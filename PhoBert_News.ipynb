{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "# !mkdir ./content/drive/MyDrive/News\n",
        "os.chdir('/content/drive/MyDrive/News/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjcGQlXlqu8k",
        "outputId": "402bb402-ef81-4dd6-8521-58448d4bf4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/dangvansam98/phobert-text-classification.git\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/Covid_Sentiment/phobert-text-classification')"
      ],
      "metadata": {
        "id": "lkC3jBZY1136"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM, Input, GlobalAveragePooling1D, Flatten\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "!pip install vncorenlp\n",
        "# !mkdir -p vncorenlp/models/wordsegmenter  \n",
        "# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar  \n",
        "# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab  \n",
        "# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr  \n",
        "# !mv VnCoreNLP-1.1.1.jar vncorenlp/   \n",
        "# !mv vi-vocab vncorenlp/models/wordsegmenter/  \n",
        "# !mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poqMovjG2Eq3",
        "outputId": "37da9e96-cb93-44c9-b94e-b3f00ea52a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645951 sha256=8281429a405f703a58297d2b711c2ceddb9a5cd5b730745481acfb638f966964\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/d8/f2/d28d97379b4f6479bf51247c8dfd57fa00932fa7a74b6aab29\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HoH7IKn2meb",
        "outputId": "fd11e035-a23e-485d-cbff-18c6ca1bdcb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.0 tokenizers-0.12.1 transformers-4.22.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
        "\n",
        "# !tar -xf PhoBERT_base_transformers.tar.gz\n",
        "# !rm -rf /content/drive/MyDrive/Covid_Sentiment/phobert-text-classification/vinai\n"
      ],
      "metadata": {
        "id": "xxJ0Yja33Oay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# !python /content/drive/MyDrive/Covid_Sentiment/phobert-text-classification/train_transformers_classifier_pytorch.py"
      ],
      "metadata": {
        "id": "bdm7Og_W4Xub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import os\n",
        "from transformers import RobertaForSequenceClassification, RobertaConfig, AdamW, RobertaTokenizer, RobertaTokenizerFast, RobertaModel, AutoTokenizer\n",
        "from datetime import datetime\n",
        "import glob\n",
        "\n",
        "def make_mask(batch_ids):\n",
        "    batch_mask = []\n",
        "    for ids in batch_ids:\n",
        "        mask = [int(token_id > 0) for token_id in ids]\n",
        "        batch_mask.append(mask)\n",
        "    return torch.tensor(batch_mask)\n",
        "\n",
        "def dataloader_from_text(text_file=None, tokenizer=None, classes=[], savetodisk=None, loadformdisk=None, segment=False, max_len=256, batch_size=16, infer=False):\n",
        "    ids_padded, masks, labels = [], [], []\n",
        "    if loadformdisk == None:\n",
        "        #segementer\n",
        "        if segment:\n",
        "            rdrsegmenter = VnCoreNLP(\"./vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "        texts = []\n",
        "        print(\"LOADDING TEXT FILE\")\n",
        "        with open(text_file, 'r') as f_r:\n",
        "            for sample in tqdm(f_r):\n",
        "                if infer:\n",
        "                    text = sample.strip()\n",
        "                    if segment:\n",
        "                        text = rdrsegmenter.tokenize(text)\n",
        "                        text = ' '.join([' '.join(x) for x in text])\n",
        "                    texts.append(text)\n",
        "                else:\n",
        "                    splits = sample.strip().split(\" \",1)\n",
        "                    label = classes.index(splits[0])\n",
        "                    text = splits[1]\n",
        "                    if segment:\n",
        "                        text = rdrsegmenter.tokenize(text)\n",
        "                        text = ' '.join([' '.join(x) for x in text])\n",
        "                    labels.append(label)\n",
        "                    texts.append(text)\n",
        "\n",
        "        print(\"TEXT TO IDS\")\n",
        "        ids = []\n",
        "        for text in tqdm(texts):\n",
        "            encoded_sent = tokenizer.encode(text)\n",
        "            ids.append(encoded_sent)\n",
        "\n",
        "        del texts\n",
        "        # print(\"PADDING IDS\")\n",
        "        ids_padded = pad_sequences(ids, maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "        del ids\n",
        "        # print(\"CREATE MASK\")\n",
        "        # for sent in tqdm(ids_padded):\n",
        "        #     masks.append(make_mask(sent))\n",
        "\n",
        "        if savetodisk != None and not infer:\n",
        "            with open(savetodisk, 'wb') as f:\n",
        "                pickle.dump(ids_padded, f)\n",
        "                # pickle.dump(masks, f)\n",
        "                pickle.dump(labels, f)\n",
        "            print(\"SAVED IDS DATA TO DISK\")\n",
        "    else:\n",
        "        print(\"LOAD FORM DISK\")\n",
        "        if loadformdisk != None:\n",
        "            try:\n",
        "                with open(savetodisk, 'rb') as f:\n",
        "                    ids_padded = pickle.load(ids_padded, f)\n",
        "                    # masks = pickle.load(masks, f)\n",
        "                    labels = pickle.load(labels, f)\n",
        "                print(\"LOADED IDS DATA FORM DISK\")\n",
        "            except:\n",
        "                print(\"LOAD DATA FORM DISK ERROR!\")\n",
        "                \n",
        "    print(\"CONVERT TO TORCH TENSOR\")\n",
        "    ids_inputs = torch.tensor(ids_padded)\n",
        "    del ids_padded\n",
        "    # masks = torch.tensor(masks)\n",
        "    if not infer:\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    print(\"CREATE DATALOADER\")\n",
        "    if infer:\n",
        "        # input_data = TensorDataset(ids_inputs, masks)\n",
        "        input_data = TensorDataset(ids_inputs)\n",
        "    else:\n",
        "        input_data = TensorDataset(ids_inputs, labels)\n",
        "        # input_data = TensorDataset(ids_inputs, masks, labels)\n",
        "    input_sampler = SequentialSampler(input_data)\n",
        "    dataloader = DataLoader(input_data, sampler=input_sampler, batch_size=batch_size)\n",
        "\n",
        "    print(\"len dataloader:\", len(dataloader))\n",
        "    print(\"LOAD DATA ALL DONE\")\n",
        "    return dataloader\n",
        "\n",
        "class ROBERTAClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_labels, bert_model, dropout_rate=0.3):\n",
        "        super(ROBERTAClassifier, self).__init__()\n",
        "        if bert_model != None:\n",
        "            self.roberta = bert_model\n",
        "        else:\n",
        "            self.roberta = RobertaModel.from_pretrained(\"./vinai/phobert-base\")\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(768, 64)\n",
        "        self.bn1 = torch.nn.LayerNorm(64)\n",
        "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l2 = torch.nn.Linear(64, num_labels)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        x = self.d1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.nn.Tanh()(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.l2(x)\n",
        "        return x \n",
        "\n",
        "class BERTClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        bert_classifier_config = RobertaConfig.from_pretrained(\n",
        "            \"./vinai/phobert-base/config.json\",\n",
        "            from_tf=False,\n",
        "            num_labels = num_labels,\n",
        "            output_hidden_states=False,\n",
        "            )\n",
        "        print(\"LOAD BERT PRETRAIN MODEL\")\n",
        "        self.bert_classifier = RobertaForSequenceClassification.from_pretrained(\n",
        "            \"./vinai/phobert-base/pytorch_model.bin\",\n",
        "            config=bert_classifier_config\n",
        "            )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels):\n",
        "        output = self.bert_classifier(input_ids=input_ids,\n",
        "                                    token_type_ids=None,\n",
        "                                    attention_mask=attention_mask,\n",
        "                                    labels=labels\n",
        "                                    )\n",
        "        return output\n",
        "\n",
        "class ClassifierTrainner():\n",
        "    def __init__(self, bert_model, train_dataloader, valid_dataloader, epochs=10, cuda_device=\"cpu\", save_dir=None):\n",
        "\n",
        "        if cuda_device == \"cpu\":\n",
        "            self.device == torch.device(\"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device('cuda:{}'.format(cuda_device))\n",
        "\n",
        "        self.model = bert_model\n",
        "        if save_dir != None and os.path.exists(save_dir):\n",
        "            print(\"Load weight from file:{}\".format(save_dir))\n",
        "            self.save_dir = save_dir\n",
        "            epcho_checkpoint_path = glob.glob(\"{}/model_epoch*\".format(self.save_dir))\n",
        "            if len(epcho_checkpoint_path) == 0:\n",
        "                print(\"No checkpoint found in: {}\\nCheck save_dir...\".format(self.save_dir))\n",
        "            else:\n",
        "                self.load_checkpoint(epcho_checkpoint_path)\n",
        "                print(\"Restore weight successful from: {}\".format(epcho_checkpoint_path))\n",
        "        else:\n",
        "            self.save_dir = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "            os.makedirs(self.save_dir)\n",
        "            print(\"Training new model, save to: {}\".format(self.save_dir))\n",
        "\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.valid_dataloader = valid_dataloader\n",
        "        self.epochs = epochs\n",
        "        # self.batch_size = batch_size\n",
        "\n",
        "    def save_checkpoint(self, save_path):\n",
        "        state_dict = {'model_state_dict': self.model.state_dict()}\n",
        "        torch.save(state_dict, save_path)\n",
        "        print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "    def load_checkpoint(self, load_path):\n",
        "        state_dict = torch.load(load_path, map_location=self.device)\n",
        "        print(f'Model restored from <== {load_path}')\n",
        "        self.model.load_state_dict(state_dict['model_state_dict'])\n",
        "\n",
        "    @staticmethod    \n",
        "    def flat_accuracy(preds, labels):\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        labels_flat = labels.flatten()\n",
        "        F1_score = f1_score(pred_flat, labels_flat, average='macro')\n",
        "        return accuracy_score(pred_flat, labels_flat), F1_score\n",
        "\n",
        "    def train_classifier(self):\n",
        "        self.model.to(self.device)\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "            ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, correct_bias=False)\n",
        "        loss_train_list = []\n",
        "        acurracy_train_list = []\n",
        "        acurracy_val_list = []\n",
        "        loss_val_list = []\n",
        "        best_valid_loss = 999999\n",
        "        best_eval_accuracy = 0\n",
        "                \n",
        "        for epoch_i in range(0, self.epochs):\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, self.epochs))\n",
        "            print('Training...')\n",
        "            total_loss = 0\n",
        "            train_accuracy = 0\n",
        "            train_f1 = 0\n",
        "            nb_train_steps = 0\n",
        "            self.model.train()\n",
        "            \n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "                b_input_ids = batch[0].to(self.device)\n",
        "                b_input_mask = make_mask(batch[0]).to(self.device)\n",
        "                b_labels = batch[1].to(self.device)\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels\n",
        "                                    )\n",
        "                loss = outputs[0]\n",
        "                total_loss += loss.item()\n",
        "                \n",
        "                logits = outputs[1].detach().cpu().numpy()\n",
        "                label_ids = b_labels.cpu().numpy()\n",
        "                tmp_train_accuracy, tmp_train_f1 = self.flat_accuracy(logits, label_ids)\n",
        "                train_accuracy += tmp_train_accuracy\n",
        "                train_f1 += tmp_train_f1\n",
        "                nb_train_steps += 1\n",
        "                \n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                print(\"[TRAIN] Epoch {}/{} | Batch {}/{} | Train Loss={} | Train Acc={}\".format(epoch_i+1, self.epochs, step, len(self.train_dataloader), loss.item(), tmp_train_accuracy))\n",
        "                \n",
        "                if step % 100 == 0:\n",
        "                    if step > 0:\n",
        "                      accur_val, loss_val = self.validate_classifier(epoch_i+1, best_valid_loss, best_eval_accuracy)\n",
        "                      best_eval_accuracy = accur_val\n",
        "                      best_valid_loss = loss_val\n",
        "                      acurracy_val_list.append(accur_val)\n",
        "                      loss_val_list.append(loss_val)\n",
        "\n",
        "                      avg_train_loss = total_loss / len(self.train_dataloader)\n",
        "                      acurracy_each_steps = train_accuracy/nb_train_steps\n",
        "                      print(\" Train Accuracy: {0:.4f}\".format(acurracy_each_steps))\n",
        "                      acurracy_train_list.append(acurracy_each_steps)\n",
        "                      print(\" Train F1 score: {0:.4f}\".format(train_f1/nb_train_steps))\n",
        "                      print(\" Train Loss: {0:.4f}\".format(avg_train_loss))\n",
        "                      loss_train_list.append(avg_train_loss)\n",
        "            \n",
        "        print(\"Training complete!\")\n",
        "        return acurracy_train_list, loss_train_list, acurracy_val_list, loss_val_list\n",
        "\n",
        "    def validate_classifier(self, currentEpoch, best_valid_loss, best_eval_accuracy):\n",
        "        print(\"Running Validation...\")\n",
        "        self.model.eval()\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "        eval_f1 = 0\n",
        "        for batch in self.valid_dataloader:\n",
        "            b_input_mask = make_mask(batch[0]).to(self.device)\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            b_input_ids, b_labels = batch\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels\n",
        "                                    )\n",
        "                tmp_eval_loss, logits = outputs[0], outputs[1]\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.cpu().numpy()\n",
        "                tmp_eval_accuracy, tmp_eval_f1 = self.flat_accuracy(logits, label_ids)\n",
        "                eval_accuracy += tmp_eval_accuracy\n",
        "                eval_loss += tmp_eval_loss\n",
        "                eval_f1 += tmp_eval_f1\n",
        "                nb_eval_steps += 1\n",
        "\n",
        "        loss_val = eval_loss/nb_eval_steps\n",
        "        print(\" Valid Loss: {0:.4f}\".format(loss_val))\n",
        "        accur_val = eval_accuracy/nb_eval_steps\n",
        "        print(\" Valid Accuracy: {0:.4f}\".format(accur_val))\n",
        "        print(\" Valid F1 score: {0:.4f}\".format(eval_f1/nb_eval_steps))\n",
        "        \n",
        "        print(\"best_valid_loss = {0:.4f}\".format(best_valid_loss))\n",
        "        print(\"eval_loss = {0:.4f}\".format(loss_val))\n",
        "\n",
        "        if eval_loss > best_valid_loss:\n",
        "            best_valid_loss_path = \"{}/model_best_valoss_{}epochs.pth\".format(self.save_dir, self.epochs)\n",
        "            self.save_checkpoint(best_valid_loss_path)\n",
        "        \n",
        "        print(\"best_eval_accuracy = {0:.4f}\".format(best_eval_accuracy))\n",
        "        print(\"eval_accuracy = {0:.4f}\".format(accur_val))\n",
        "\n",
        "        if accur_val > best_eval_accuracy:\n",
        "            best_eval_accuracy_path = \"{}/model_best_valacc{}epochs.pth\".format(self.save_dir, self.epochs)\n",
        "            self.save_checkpoint(best_eval_accuracy_path)\n",
        "        \n",
        "        if currentEpoch == self.epochs:\n",
        "            epoch_path = \"{}/model_{}epoch.pth\".format(self.save_dir, currentEpoch)\n",
        "            self.save_checkpoint(epoch_path)\n",
        "            # os.remove(\"{}/model_{}epoch.pth\".format(self.save_dir, currentEpoch-1))\n",
        "\n",
        "        return accur_val, loss_val.item()\n",
        "\n",
        "    def plot_model(self, acurracy_train_list, loss_train_list, acurracy_val_list, loss_val_list):\n",
        "      # list all data in history\n",
        "      # summarize history for accuracy\n",
        "      # import matplotlib \n",
        "      plt.plot(acurracy_train_list)\n",
        "      plt.plot(acurracy_val_list)\n",
        "      # formatter = matplotlib.ticker.StrMethodFormatter(\"{x:.3f}\")\n",
        "      # plt.gca().yaxis.set_major_formatter(formatter)\n",
        "      plt.title('model accuracy')\n",
        "      plt.ylabel('accuracy')\n",
        "      plt.xlabel('epoch')\n",
        "      plt.legend(['train', 'vald'], loc='upper left')\n",
        "      plt.show()\n",
        "      # summarize history for loss\n",
        "      plt.plot(loss_train_list)\n",
        "      plt.plot(loss_val_list)\n",
        "      plt.title('model loss')\n",
        "      plt.ylabel('loss')\n",
        "      plt.xlabel('epoch')\n",
        "      plt.legend(['train', 'vald'], loc='upper left')\n",
        "      plt.show()\n",
        "\n",
        "    def predict_dataloader(self, dataloader, classes, tokenizer):\n",
        "        for batch in dataloader:\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            b_input_ids, b_input_mask = batch\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=None\n",
        "                                    )\n",
        "                logits = outputs\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "                print(\"[PREDICT] {}:{}\".format(classes[int(pred_flat)], tokenizer.decode(b_input_ids)))\n",
        "\n",
        "    def predict_text(self, text, classes, tokenizer, max_len=256):\n",
        "      self.model.cuda()\n",
        "      ids = tokenizer.encode(text)\n",
        "      ids_padded = pad_sequences([ids], maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "      mask = [[int(token_id > 0)] for token_id in ids_padded[0]]\n",
        "      input_ids = torch.tensor(ids_padded)\n",
        "      input_mask = torch.tensor(mask)\n",
        "      with torch.no_grad():\n",
        "          logits = self.model(input_ids.cuda(), \n",
        "                          attention_mask=input_mask.cuda(),\n",
        "                          labels=None)[0]\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "      print(\"[PREDICT] \\nTEXT: {}\\nLABEL: {}\".format(text, classes[pred_flat[1]]))\n"
      ],
      "metadata": {
        "id": "-1qBxCd8J8xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['__label__sống_trẻ', '__label__thời_sự', '__label__công_nghệ', '__label__sức_khỏe', '__label__giáo_dục', '__label__xe_360', '__label__thời_trang', '__label__du_lịch', '__label__âm_nhạc', '__label__xuất_bản', '__label__nhịp_sống', '__label__kinh_doanh', '__label__pháp_luật', '__label__ẩm_thực', '__label__thế_giới', '__label__thể_thao', '__label__giải_trí', '__label__phim_ảnh']\n",
        "\n",
        "train_path = 'train.txt'\n",
        "test_path = 'test.txt'\n",
        "\n",
        "MAX_LEN = 256\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./vinai/phobert-base\", local_files_only=True)\n",
        "\n",
        "train_dataloader = dataloader_from_text(train_path, tokenizer=tokenizer, classes=classes, savetodisk=None, max_len=MAX_LEN, batch_size=32)\n",
        "valid_dataloader = dataloader_from_text(test_path, tokenizer=tokenizer, classes=classes, savetodisk=None, max_len=MAX_LEN, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLJlMVOCWMMT",
        "outputId": "edd3e900-54a2-4707-f763-b5b90d4b182a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADDING TEXT FILE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8903it [00:00, 10077.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT TO IDS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/8903 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 256). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 8903/8903 [00:30<00:00, 292.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONVERT TO TORCH TENSOR\n",
            "CREATE DATALOADER\n",
            "len dataloader: 279\n",
            "LOAD DATA ALL DONE\n",
            "LOADDING TEXT FILE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "433it [00:00, 626.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT TO IDS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433/433 [00:01<00:00, 391.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONVERT TO TORCH TENSOR\n",
            "CREATE DATALOADER\n",
            "len dataloader: 14\n",
            "LOAD DATA ALL DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert model\n",
        "bert_classifier_model = BERTClassifier(len(classes))\n",
        "# Initialize model\n",
        "bert_classifier_trainer = ClassifierTrainner(bert_model=bert_classifier_model, \n",
        "                                             train_dataloader=train_dataloader, \n",
        "                                             valid_dataloader=valid_dataloader, \n",
        "                                             epochs=5, \n",
        "                                             cuda_device=\"0\") #cuda_device: \"cpu\"=cpu hoac 0=gpu0, 1=gpu1, "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwdukqxaYloT",
        "outputId": "696db748-e2ba-45f5-c373-bf9b0c35f5c2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOAD BERT PRETRAIN MODEL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./vinai/phobert-base/pytorch_model.bin were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./vinai/phobert-base/pytorch_model.bin and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training new model, save to: 30-09-2022_08-46-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model \n",
        "accur_train, loss_train, accur_val, loss_val = bert_classifier_trainer.train_classifier()\n",
        "\n",
        "# Plot model\n",
        "bert_classifier_trainer.plot_model(accur_train, loss_train, accur_val, loss_val)"
      ],
      "metadata": {
        "id": "orb-sBhSjgKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes = ['__label__sống_trẻ', '__label__thời_sự', '__label__công_nghệ', '__label__sức_khỏe', '__label__giáo_dục', '__label__xe_360', '__label__thời_trang', '__label__du_lịch', '__label__âm_nhạc', '__label__xuất_bản', '__label__nhịp_sống', '__label__kinh_doanh', '__label__pháp_luật', '__label__ẩm_thực', '__label__thế_giới', '__label__thể_thao', '__label__giải_trí', '__label__phim_ảnh']\n",
        "\n",
        "# Load model from saved file\n",
        "load_model = torch.load('/content/drive/MyDrive/Covid_Sentiment/phobert-text-classification/30-09-2022_06-06-52/model_5epoch.pth')\n",
        "model = BERTClassifier(len(classes))\n",
        "model.load_state_dict(load_model['model_state_dict']) # Load state dict, include input_ids and attention_mask"
      ],
      "metadata": {
        "id": "2xql3VH6MkDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c03d81-2429-4761-f8e4-894d932312b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOAD BERT PRETRAIN MODEL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./vinai/phobert-base/pytorch_model.bin were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./vinai/phobert-base/pytorch_model.bin and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Ô tô Trung Quốc vào Việt Nam từ cách đây 2 thập niên và đến nay vẫn bị mang tiếng \"nhái\" thiết kế nhiều thương hiệu tên tuổi. Nhắc đến ô tô Trung Quốc, bên cạnh chất lượng bị nghi ngờ thì trong suy nghĩ và định kiến của người Việt, nhiều mẫu xe đến từ bên kia biên giới không có bản sắc mà thường \"nhái\" theo các thương hiệu tên tuổi trên thế giới. Kể từ khi xe Trung Quốc vào Việt Nam cách đây hơn 2 thập niên và đến hiện tại, yếu tố nhái kiểu dáng vẫn còn tồn tại. Ưu điểm của những xe nhái thiết kế là có giá rẻ hơn rất nhiều so với xe thật. Cho dù công nghệ, vận hành khó có thể bằng nhưng một số mẫu ô tô nhái của Trung Quốc vẫn có phân khúc khách hàng riêng. Dưới đây là một số mẫu xe điển hình như vậy đã bán ở Việt Nam.'\n",
        "bert_classifier_trainer.predict_text(classes=classes, text=text, tokenizer=tokenizer, max_len=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbyGLYJc_cd7",
        "outputId": "aac6c8ab-d0e5-4ec1-933c-d3768af5478b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PREDICT] \n",
            "TEXT: Ô tô Trung Quốc vào Việt Nam từ cách đây 2 thập niên và đến nay vẫn bị mang tiếng \"nhái\" thiết kế nhiều thương hiệu tên tuổi. Nhắc đến ô tô Trung Quốc, bên cạnh chất lượng bị nghi ngờ thì trong suy nghĩ và định kiến của người Việt, nhiều mẫu xe đến từ bên kia biên giới không có bản sắc mà thường \"nhái\" theo các thương hiệu tên tuổi trên thế giới. Kể từ khi xe Trung Quốc vào Việt Nam cách đây hơn 2 thập niên và đến hiện tại, yếu tố nhái kiểu dáng vẫn còn tồn tại. Ưu điểm của những xe nhái thiết kế là có giá rẻ hơn rất nhiều so với xe thật. Cho dù công nghệ, vận hành khó có thể bằng nhưng một số mẫu ô tô nhái của Trung Quốc vẫn có phân khúc khách hàng riêng. Dưới đây là một số mẫu xe điển hình như vậy đã bán ở Việt Nam.\n",
            "LABEL: __label__sức_khỏe\n"
          ]
        }
      ]
    }
  ]
}