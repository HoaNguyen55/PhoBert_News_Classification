{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d4b4205a6a84836804f0d452a22b18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1390b96fa2e4151bb0772754db254aa",
              "IPY_MODEL_f532c8a7a7914b779221cc1706978e8c",
              "IPY_MODEL_ecc69e6fb2804019b3ad2e8e01a77eb6"
            ],
            "layout": "IPY_MODEL_1b3b701664a94759a506d5e37617c447"
          }
        },
        "c1390b96fa2e4151bb0772754db254aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c716662d88d0495b9b792c99e0a4c5ff",
            "placeholder": "​",
            "style": "IPY_MODEL_8180714cd2e84e98853a9f41b764a5ad",
            "value": "Downloading: 100%"
          }
        },
        "f532c8a7a7914b779221cc1706978e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2a6b290462544e5b7714ef8d240ac78",
            "max": 557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1339594dfbb4db1bb892fdd00d520e5",
            "value": 557
          }
        },
        "ecc69e6fb2804019b3ad2e8e01a77eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fea0f1d196b46eb9973a5a98b066556",
            "placeholder": "​",
            "style": "IPY_MODEL_33561a44479c4a94ae61f6a5e1494747",
            "value": " 557/557 [00:00&lt;00:00, 21.2kB/s]"
          }
        },
        "1b3b701664a94759a506d5e37617c447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c716662d88d0495b9b792c99e0a4c5ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8180714cd2e84e98853a9f41b764a5ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2a6b290462544e5b7714ef8d240ac78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1339594dfbb4db1bb892fdd00d520e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fea0f1d196b46eb9973a5a98b066556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33561a44479c4a94ae61f6a5e1494747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbff200216d54e1095b82b0e5f3a1691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42028e4b3046411093200d9c194086cb",
              "IPY_MODEL_aa42db7495b6438fa4ec6b96bacf7237",
              "IPY_MODEL_ea50535b1f7c4343a6468f74f38896ca"
            ],
            "layout": "IPY_MODEL_13f9f7d53bec402188038c144c618025"
          }
        },
        "42028e4b3046411093200d9c194086cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c567e9d7b3f497fae4bd728aa0efb25",
            "placeholder": "​",
            "style": "IPY_MODEL_835fb8d87f8d466dbeec16b4373d493f",
            "value": "Downloading: 100%"
          }
        },
        "aa42db7495b6438fa4ec6b96bacf7237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3fda32c173646ff80732773a437f980",
            "max": 542923308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c57f329a65e46b1b3404179b51b476d",
            "value": 542923308
          }
        },
        "ea50535b1f7c4343a6468f74f38896ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b80379e91e614212a5dfca16fa6e35e9",
            "placeholder": "​",
            "style": "IPY_MODEL_67666174ac354bcbaf315cd485bc6024",
            "value": " 543M/543M [00:09&lt;00:00, 44.7MB/s]"
          }
        },
        "13f9f7d53bec402188038c144c618025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c567e9d7b3f497fae4bd728aa0efb25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835fb8d87f8d466dbeec16b4373d493f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3fda32c173646ff80732773a437f980": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c57f329a65e46b1b3404179b51b476d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b80379e91e614212a5dfca16fa6e35e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67666174ac354bcbaf315cd485bc6024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "408e42825bc64bae949483b1252cc8d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cbb00232b0884743972c994da8159700",
              "IPY_MODEL_3cc4614856a04bbaa58ee087afe454ee",
              "IPY_MODEL_2d98b2e0fd26475086aa03775b57e4fd"
            ],
            "layout": "IPY_MODEL_473dfa87df0e40b693937f7d6d0cf226"
          }
        },
        "cbb00232b0884743972c994da8159700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e602d814e7f84625a79bb7f8bc447a5f",
            "placeholder": "​",
            "style": "IPY_MODEL_b41d1878fa974c9abb486dc3a79c8c14",
            "value": "Downloading: 100%"
          }
        },
        "3cc4614856a04bbaa58ee087afe454ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49d4dc51f4c34691b47e81f7868f7815",
            "max": 1135173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_671b9506623d43cb805de88206f8edfd",
            "value": 1135173
          }
        },
        "2d98b2e0fd26475086aa03775b57e4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89584653839e40038590321f95c88d25",
            "placeholder": "​",
            "style": "IPY_MODEL_c26e263d193a4f60a6f955fb556c5148",
            "value": " 1.14M/1.14M [00:00&lt;00:00, 2.82MB/s]"
          }
        },
        "473dfa87df0e40b693937f7d6d0cf226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e602d814e7f84625a79bb7f8bc447a5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b41d1878fa974c9abb486dc3a79c8c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49d4dc51f4c34691b47e81f7868f7815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "671b9506623d43cb805de88206f8edfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89584653839e40038590321f95c88d25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c26e263d193a4f60a6f955fb556c5148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad0c4788ea2c41e58f9fea9e22ba08f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afa4d598cd394eee9c96dc56f67054b8",
              "IPY_MODEL_9f1919832dca44ee9fa44591a4db6b2d",
              "IPY_MODEL_ade32eda99544aceab70c0a0fafb13e4"
            ],
            "layout": "IPY_MODEL_a11d80627b9143c1bdf804e1302cb28c"
          }
        },
        "afa4d598cd394eee9c96dc56f67054b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dbfb853b4e044d0a6e4fb7c7c93f344",
            "placeholder": "​",
            "style": "IPY_MODEL_b30516480d6346f5a78029ae396b967a",
            "value": "Downloading: 100%"
          }
        },
        "9f1919832dca44ee9fa44591a4db6b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4ec4800a86d4a698498290ce57dd735",
            "max": 3132320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e967aa72a22d4592b9b5456d698dba2d",
            "value": 3132320
          }
        },
        "ade32eda99544aceab70c0a0fafb13e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f6735e177244c8eaa584cb6b85cc3d2",
            "placeholder": "​",
            "style": "IPY_MODEL_6160c5f3221d42df843f925b2ed60ee8",
            "value": " 3.13M/3.13M [00:00&lt;00:00, 4.63MB/s]"
          }
        },
        "a11d80627b9143c1bdf804e1302cb28c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dbfb853b4e044d0a6e4fb7c7c93f344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b30516480d6346f5a78029ae396b967a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4ec4800a86d4a698498290ce57dd735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e967aa72a22d4592b9b5456d698dba2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f6735e177244c8eaa584cb6b85cc3d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6160c5f3221d42df843f925b2ed60ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "851414b2348d45a9b8d4528e2cd6c1a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8bb8fb19b7d43aa9a31952f439d7350",
              "IPY_MODEL_a06dab62469e4d808882ee25397e25e3",
              "IPY_MODEL_2899a1a8a9af4c41b3b51115d6df61f7"
            ],
            "layout": "IPY_MODEL_0e5d46fa954f4957b702f555de7eccc8"
          }
        },
        "c8bb8fb19b7d43aa9a31952f439d7350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47ed96942411497db29dcb3daddc9494",
            "placeholder": "​",
            "style": "IPY_MODEL_ac9919ef050148d9889cf5ac80adc8ab",
            "value": "Downloading: 100%"
          }
        },
        "a06dab62469e4d808882ee25397e25e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f318018f29054af3b85e0f477c628776",
            "max": 895321,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eeb3ba5dd2ea401f86db844c1bbd210d",
            "value": 895321
          }
        },
        "2899a1a8a9af4c41b3b51115d6df61f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ad4967acab74a768864b581b1cddbc0",
            "placeholder": "​",
            "style": "IPY_MODEL_51a33c90016a493eb6f07e1eb7cd6162",
            "value": " 895k/895k [00:00&lt;00:00, 2.31MB/s]"
          }
        },
        "0e5d46fa954f4957b702f555de7eccc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47ed96942411497db29dcb3daddc9494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac9919ef050148d9889cf5ac80adc8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f318018f29054af3b85e0f477c628776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb3ba5dd2ea401f86db844c1bbd210d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ad4967acab74a768864b581b1cddbc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51a33c90016a493eb6f07e1eb7cd6162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoaNguyen55/PhoBert_News_Classification/blob/main/PhoBert_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "!mkdir /content/drive/MyDrive/News\n",
        "os.chdir('/content/drive/MyDrive/News/')\n"
      ],
      "metadata": {
        "id": "zjcGQlXlqu8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3355fcb6-145e-4cb0-fb99-c0e14bae53b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HoaNguyen55/PhoBert_News_Classification.git\n",
        "os.chdir('/content/drive/MyDrive/News/PhoBert_News_Classification')"
      ],
      "metadata": {
        "id": "lkC3jBZY1136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ea9bef-f593-4068-9945-c05510424aad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PhoBert_News_Classification'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 36 (delta 14), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (36/36), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM, Input, GlobalAveragePooling1D, Flatten\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "!pip install vncorenlp\n",
        "!mkdir -p vncorenlp/models/wordsegmenter  \n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar  \n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab  \n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr  \n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/   \n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/  \n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poqMovjG2Eq3",
        "outputId": "320ba16c-2fd3-4050-ad74-391df5a0df2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2022.6.15)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645951 sha256=c36b50d9c7a544a0398f2d487e7d93b838e651a8620ef61df6c60d7b87a5c46b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/d8/f2/d28d97379b4f6479bf51247c8dfd57fa00932fa7a74b6aab29\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n",
            "--2022-10-03 07:22:04--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  83.3MB/s    in 0.3s    \n",
            "\n",
            "2022-10-03 07:22:06 (83.3 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2022-10-03 07:22:06--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-10-03 07:22:06 (17.0 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2022-10-03 07:22:06--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-10-03 07:22:06 (10.6 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HoH7IKn2meb",
        "outputId": "c6a1170b-9e81-4b3a-832d-dffbe9e24f3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 89.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 70.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.0 tokenizers-0.12.1 transformers-4.22.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
        "# !tar -xvf PhoBERT_base_transformers.tar.gz -C /content/drive/MyDrive/Covid_Sentiment/phobert-text-classification/vinai/phobert-base\n",
        "# !mv ./vinai/phobert-base/PhoBERT_base_transformers/* ./vinai/phobert-base/.\n",
        "# !rm -rf ./vinai/phobert-base/PhoBERT_base_transformers\n",
        "# !mv ./vinai/phobert-base/model.bin ./vinai/phobert-base/pytorch_model.bin\n",
        "\n",
        "# !wget https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
        "# !tar -xvf PhoBERT_base_transformers.tar.gz -C /content/drive/MyDrive/Covid_Sentiment/phobert-text-classification/vinai/phobert-base\n",
        "# !mv ./vinai/phobert-base/PhoBERT_base_transformers/* ./vinai/phobert-base/.\n",
        "# !rm -rf ./vinai/phobert-base/PhoBERT_base_transformers/\n",
        "# !mv ./vinai/phobert-base/model.bin ./vinai/phobert-base/pytorch_model.bin"
      ],
      "metadata": {
        "id": "xxJ0Yja33Oay"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from vncorenlp import VnCoreNLP\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import os\n",
        "from transformers import RobertaForSequenceClassification, RobertaConfig, AdamW, RobertaTokenizer, RobertaTokenizerFast, RobertaModel, AutoTokenizer\n",
        "from datetime import datetime\n",
        "import glob\n",
        "\n",
        "def make_mask(batch_ids):\n",
        "    batch_mask = []\n",
        "    for ids in batch_ids:\n",
        "        mask = [int(token_id > 0) for token_id in ids]\n",
        "        batch_mask.append(mask)\n",
        "    return torch.tensor(batch_mask)\n",
        "\n",
        "def dataloader_from_text(text_file=None, tokenizer=None, classes=[], savetodisk=None, loadformdisk=None, segment=False, max_len=256, batch_size=16, infer=False):\n",
        "    ids_padded, masks, labels = [], [], []\n",
        "    if loadformdisk == None:\n",
        "        #segementer\n",
        "        if segment:\n",
        "            rdrsegmenter = VnCoreNLP(\"./vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "        texts = []\n",
        "        print(\"LOADDING TEXT FILE\")\n",
        "        with open(text_file, 'r') as f_r:\n",
        "            for sample in tqdm(f_r):\n",
        "                if infer:\n",
        "                    text = sample.strip()\n",
        "                    if segment:\n",
        "                        text = rdrsegmenter.tokenize(text)\n",
        "                        text = ' '.join([' '.join(x) for x in text])\n",
        "                    texts.append(text)\n",
        "                else:\n",
        "                    splits = sample.strip().split(\" \",1)\n",
        "                    label = classes.index(splits[0])\n",
        "                    text = splits[1]\n",
        "                    if segment:\n",
        "                        text = rdrsegmenter.tokenize(text)\n",
        "                        text = ' '.join([' '.join(x) for x in text])\n",
        "                    labels.append(label)\n",
        "                    texts.append(text)\n",
        "\n",
        "        print(\"TEXT TO IDS\")\n",
        "        ids = []\n",
        "        for text in tqdm(texts):\n",
        "            encoded_sent = tokenizer.encode(text)\n",
        "            ids.append(encoded_sent)\n",
        "\n",
        "        del texts\n",
        "        # print(\"PADDING IDS\")\n",
        "        ids_padded = pad_sequences(ids, maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "        del ids\n",
        "        # print(\"CREATE MASK\")\n",
        "        # for sent in tqdm(ids_padded):\n",
        "        #     masks.append(make_mask(sent))\n",
        "\n",
        "        if savetodisk != None and not infer:\n",
        "            with open(savetodisk, 'wb') as f:\n",
        "                pickle.dump(ids_padded, f)\n",
        "                # pickle.dump(masks, f)\n",
        "                pickle.dump(labels, f)\n",
        "            print(\"SAVED IDS DATA TO DISK\")\n",
        "    else:\n",
        "        print(\"LOAD FORM DISK\")\n",
        "        if loadformdisk != None:\n",
        "            try:\n",
        "                with open(savetodisk, 'rb') as f:\n",
        "                    ids_padded = pickle.load(ids_padded, f)\n",
        "                    # masks = pickle.load(masks, f)\n",
        "                    labels = pickle.load(labels, f)\n",
        "                print(\"LOADED IDS DATA FORM DISK\")\n",
        "            except:\n",
        "                print(\"LOAD DATA FORM DISK ERROR!\")\n",
        "                \n",
        "    print(\"CONVERT TO TORCH TENSOR\")\n",
        "    ids_inputs = torch.tensor(ids_padded)\n",
        "    del ids_padded\n",
        "    # masks = torch.tensor(masks)\n",
        "    if not infer:\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    print(\"CREATE DATALOADER\")\n",
        "    if infer:\n",
        "        # input_data = TensorDataset(ids_inputs, masks)\n",
        "        input_data = TensorDataset(ids_inputs)\n",
        "    else:\n",
        "        input_data = TensorDataset(ids_inputs, labels)\n",
        "        # input_data = TensorDataset(ids_inputs, masks, labels)\n",
        "    input_sampler = SequentialSampler(input_data)\n",
        "    dataloader = DataLoader(input_data, sampler=input_sampler, batch_size=batch_size)\n",
        "\n",
        "    print(\"len dataloader:\", len(dataloader))\n",
        "    print(\"LOAD DATA ALL DONE\")\n",
        "    return dataloader\n",
        "\n",
        "class ROBERTAClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_labels, bert_model, dropout_rate=0.3):\n",
        "        super(ROBERTAClassifier, self).__init__()\n",
        "        if bert_model != None:\n",
        "            self.roberta = bert_model\n",
        "        else:\n",
        "            self.roberta = RobertaModel.from_pretrained(\"./vinai/phobert-base\")\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(768, 64)\n",
        "        self.bn1 = torch.nn.LayerNorm(64)\n",
        "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l2 = torch.nn.Linear(64, num_labels)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        x = self.d1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.nn.Tanh()(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.l2(x)\n",
        "        return x \n",
        "\n",
        "class BERTClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        bert_classifier_config = RobertaConfig.from_pretrained(\n",
        "            \"./vinai/phobert-base/config.json\",\n",
        "            from_tf=False,\n",
        "            num_labels = num_labels,\n",
        "            output_hidden_states=False,\n",
        "            )\n",
        "        print(\"LOAD BERT PRETRAIN MODEL\")\n",
        "        self.bert_classifier = RobertaForSequenceClassification.from_pretrained(\n",
        "            \"./vinai/phobert-base/pytorch_model.bin\",\n",
        "            config=bert_classifier_config\n",
        "            )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels):\n",
        "        output = self.bert_classifier(input_ids=input_ids,\n",
        "                                    token_type_ids=None,\n",
        "                                    attention_mask=attention_mask,\n",
        "                                    labels=labels\n",
        "                                    )\n",
        "        return output\n",
        "\n",
        "class ClassifierTrainner():\n",
        "    def __init__(self, bert_model, train_dataloader, valid_dataloader, epochs=10, cuda_device=\"cpu\", save_dir=None):\n",
        "\n",
        "        if cuda_device == \"cpu\":\n",
        "            self.device == torch.device(\"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device('cuda:{}'.format(cuda_device))\n",
        "\n",
        "        self.model = bert_model\n",
        "        if save_dir != None and os.path.exists(save_dir):\n",
        "            print(\"Load weight from file:{}\".format(save_dir))\n",
        "            self.save_dir = save_dir\n",
        "            epcho_checkpoint_path = glob.glob(\"{}/model_epoch*\".format(self.save_dir))\n",
        "            if len(epcho_checkpoint_path) == 0:\n",
        "                print(\"No checkpoint found in: {}\\nCheck save_dir...\".format(self.save_dir))\n",
        "            else:\n",
        "                self.load_checkpoint(epcho_checkpoint_path)\n",
        "                print(\"Restore weight successful from: {}\".format(epcho_checkpoint_path))\n",
        "        else:\n",
        "            self.save_dir = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "            os.makedirs(self.save_dir)\n",
        "            print(\"Training new model, save to: {}\".format(self.save_dir))\n",
        "\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.valid_dataloader = valid_dataloader\n",
        "        self.epochs = epochs\n",
        "        # self.batch_size = batch_size\n",
        "\n",
        "    def save_checkpoint(self, save_path):\n",
        "        state_dict = {'model_state_dict': self.model.state_dict()}\n",
        "        torch.save(state_dict, save_path)\n",
        "        print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "    def load_checkpoint(self, load_path):\n",
        "        state_dict = torch.load(load_path, map_location=self.device)\n",
        "        print(f'Model restored from <== {load_path}')\n",
        "        self.model.load_state_dict(state_dict['model_state_dict'])\n",
        "\n",
        "    @staticmethod    \n",
        "    def flat_accuracy(preds, labels):\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        labels_flat = labels.flatten()\n",
        "        F1_score = f1_score(pred_flat, labels_flat, average='macro')\n",
        "        return accuracy_score(pred_flat, labels_flat), F1_score\n",
        "\n",
        "    def train_classifier(self):\n",
        "        self.model.to(self.device)\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "            ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, correct_bias=False)\n",
        "        loss_train_list = []\n",
        "        acurracy_train_list = []\n",
        "        acurracy_val_list = []\n",
        "        loss_val_list = []\n",
        "        best_valid_loss = 999999\n",
        "        best_eval_accuracy = 0\n",
        "                \n",
        "        for epoch_i in range(0, self.epochs):\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, self.epochs))\n",
        "            print('Training...')\n",
        "            total_loss = 0\n",
        "            train_accuracy = 0\n",
        "            train_f1 = 0\n",
        "            nb_train_steps = 0\n",
        "            self.model.train()\n",
        "            \n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "                b_input_ids = batch[0].to(self.device)\n",
        "                b_input_mask = make_mask(batch[0]).to(self.device)\n",
        "                b_labels = batch[1].to(self.device)\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels\n",
        "                                    )\n",
        "                loss = outputs[0]\n",
        "                total_loss += loss.item()\n",
        "                \n",
        "                logits = outputs[1].detach().cpu().numpy()\n",
        "                label_ids = b_labels.cpu().numpy()\n",
        "                tmp_train_accuracy, tmp_train_f1 = self.flat_accuracy(logits, label_ids)\n",
        "                train_accuracy += tmp_train_accuracy\n",
        "                train_f1 += tmp_train_f1\n",
        "                nb_train_steps += 1\n",
        "                \n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                print(\"[TRAIN] Epoch {}/{} | Batch {}/{} | Train Loss={} | Train Acc={}\".format(epoch_i+1, self.epochs, step, len(self.train_dataloader), loss.item(), tmp_train_accuracy))\n",
        "                \n",
        "                if step % 100 == 0:\n",
        "                    if step > 0:\n",
        "                      accur_val, loss_val = self.validate_classifier(epoch_i+1, best_valid_loss, best_eval_accuracy)\n",
        "                      best_eval_accuracy = accur_val\n",
        "                      best_valid_loss = loss_val\n",
        "                      acurracy_val_list.append(accur_val)\n",
        "                      loss_val_list.append(loss_val)\n",
        "\n",
        "                      avg_train_loss = total_loss / len(self.train_dataloader)\n",
        "                      acurracy_each_steps = train_accuracy/nb_train_steps\n",
        "                      print(\" Train Accuracy: {0:.4f}\".format(acurracy_each_steps))\n",
        "                      acurracy_train_list.append(acurracy_each_steps)\n",
        "                      print(\" Train F1 score: {0:.4f}\".format(train_f1/nb_train_steps))\n",
        "                      print(\" Train Loss: {0:.4f}\".format(avg_train_loss))\n",
        "                      loss_train_list.append(avg_train_loss)\n",
        "            \n",
        "        print(\"Training complete!\")\n",
        "        return acurracy_train_list, loss_train_list, acurracy_val_list, loss_val_list\n",
        "\n",
        "    def validate_classifier(self, currentEpoch, best_valid_loss, best_eval_accuracy):\n",
        "        print(\"Running Validation...\")\n",
        "        self.model.eval()\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "        eval_f1 = 0\n",
        "        for batch in self.valid_dataloader:\n",
        "            b_input_mask = make_mask(batch[0]).to(self.device)\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            b_input_ids, b_labels = batch\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels\n",
        "                                    )\n",
        "                tmp_eval_loss, logits = outputs[0], outputs[1]\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.cpu().numpy()\n",
        "                tmp_eval_accuracy, tmp_eval_f1 = self.flat_accuracy(logits, label_ids)\n",
        "                eval_accuracy += tmp_eval_accuracy\n",
        "                eval_loss += tmp_eval_loss\n",
        "                eval_f1 += tmp_eval_f1\n",
        "                nb_eval_steps += 1\n",
        "\n",
        "        loss_val = eval_loss/nb_eval_steps\n",
        "        print(\" Valid Loss: {0:.4f}\".format(loss_val))\n",
        "        accur_val = eval_accuracy/nb_eval_steps\n",
        "        print(\" Valid Accuracy: {0:.4f}\".format(accur_val))\n",
        "        print(\" Valid F1 score: {0:.4f}\".format(eval_f1/nb_eval_steps))\n",
        "        \n",
        "        print(\"best_valid_loss = {0:.4f}\".format(best_valid_loss))\n",
        "        print(\"eval_loss = {0:.4f}\".format(loss_val))\n",
        "\n",
        "        if eval_loss > best_valid_loss:\n",
        "            best_valid_loss_path = \"{}/model_best_valoss_{}epochs.pth\".format(self.save_dir, self.epochs)\n",
        "            self.save_checkpoint(best_valid_loss_path)\n",
        "        \n",
        "        print(\"best_eval_accuracy = {0:.4f}\".format(best_eval_accuracy))\n",
        "        print(\"eval_accuracy = {0:.4f}\".format(accur_val))\n",
        "\n",
        "        if accur_val > best_eval_accuracy:\n",
        "            best_eval_accuracy_path = \"{}/model_best_valacc{}epochs.pth\".format(self.save_dir, self.epochs)\n",
        "            self.save_checkpoint(best_eval_accuracy_path)\n",
        "        \n",
        "        if currentEpoch == self.epochs:\n",
        "            epoch_path = \"{}/model_{}epoch.pth\".format(self.save_dir, currentEpoch)\n",
        "            self.save_checkpoint(epoch_path)\n",
        "            # os.remove(\"{}/model_{}epoch.pth\".format(self.save_dir, currentEpoch-1))\n",
        "\n",
        "        return accur_val, loss_val.item()\n",
        "\n",
        "    def plot_model(self, acurracy_train_list, loss_train_list, acurracy_val_list, loss_val_list):\n",
        "      # list all data in history\n",
        "      # summarize history for accuracy\n",
        "      # import matplotlib \n",
        "      plt.plot(acurracy_train_list)\n",
        "      plt.plot(acurracy_val_list)\n",
        "      # formatter = matplotlib.ticker.StrMethodFormatter(\"{x:.3f}\")\n",
        "      # plt.gca().yaxis.set_major_formatter(formatter)\n",
        "      plt.title('model accuracy')\n",
        "      plt.ylabel('accuracy')\n",
        "      plt.xlabel('epoch')\n",
        "      plt.legend(['train', 'vald'], loc='upper left')\n",
        "      plt.show()\n",
        "      # summarize history for loss\n",
        "      plt.plot(loss_train_list)\n",
        "      plt.plot(loss_val_list)\n",
        "      plt.title('model loss')\n",
        "      plt.ylabel('loss')\n",
        "      plt.xlabel('epoch')\n",
        "      plt.legend(['train', 'vald'], loc='upper left')\n",
        "      plt.show()\n",
        "\n",
        "    def predict_dataloader(self, dataloader, classes, tokenizer):\n",
        "        for batch in dataloader:\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            b_input_ids, b_input_mask = batch\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=None\n",
        "                                    )\n",
        "                logits = outputs\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "                print(\"[PREDICT] {}:{}\".format(classes[int(pred_flat)], tokenizer.decode(b_input_ids)))\n",
        "\n",
        "    def predict_text(self, text, classes, tokenizer, max_len=256):\n",
        "      self.model.cuda()\n",
        "      ids = tokenizer.encode(text)\n",
        "      ids_padded = pad_sequences([ids], maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "      mask = [[int(token_id > 0)] for token_id in ids_padded[0]]\n",
        "      input_ids = torch.tensor(ids_padded)\n",
        "      input_mask = torch.tensor(mask)\n",
        "      with torch.no_grad():\n",
        "          logits = self.model(input_ids.cuda(), \n",
        "                          attention_mask=input_mask.cuda(),\n",
        "                          labels=None)[0]\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "      print(\"[PREDICT] \\nTEXT: {}\\nLABEL: {}\".format(text, classes[pred_flat[1]]))\n"
      ],
      "metadata": {
        "id": "-1qBxCd8J8xd",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "# snapshot_download(repo_id=\"vinai/phobert-base\")\n",
        "hf_hub_download(repo_id=\"vinai/phobert-base\", filename=\"config.json\")\n",
        "hf_hub_download(repo_id=\"vinai/phobert-base\", filename=\"pytorch_model.bin\")\n",
        "hf_hub_download(repo_id=\"vinai/phobert-base\", filename=\"bpe.codes\")\n",
        "hf_hub_download(repo_id=\"vinai/phobert-base\", filename=\"tokenizer.json\")\n",
        "hf_hub_download(repo_id=\"vinai/phobert-base\", filename=\"vocab.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "9d4b4205a6a84836804f0d452a22b18b",
            "c1390b96fa2e4151bb0772754db254aa",
            "f532c8a7a7914b779221cc1706978e8c",
            "ecc69e6fb2804019b3ad2e8e01a77eb6",
            "1b3b701664a94759a506d5e37617c447",
            "c716662d88d0495b9b792c99e0a4c5ff",
            "8180714cd2e84e98853a9f41b764a5ad",
            "a2a6b290462544e5b7714ef8d240ac78",
            "f1339594dfbb4db1bb892fdd00d520e5",
            "2fea0f1d196b46eb9973a5a98b066556",
            "33561a44479c4a94ae61f6a5e1494747",
            "fbff200216d54e1095b82b0e5f3a1691",
            "42028e4b3046411093200d9c194086cb",
            "aa42db7495b6438fa4ec6b96bacf7237",
            "ea50535b1f7c4343a6468f74f38896ca",
            "13f9f7d53bec402188038c144c618025",
            "4c567e9d7b3f497fae4bd728aa0efb25",
            "835fb8d87f8d466dbeec16b4373d493f",
            "c3fda32c173646ff80732773a437f980",
            "4c57f329a65e46b1b3404179b51b476d",
            "b80379e91e614212a5dfca16fa6e35e9",
            "67666174ac354bcbaf315cd485bc6024",
            "408e42825bc64bae949483b1252cc8d7",
            "cbb00232b0884743972c994da8159700",
            "3cc4614856a04bbaa58ee087afe454ee",
            "2d98b2e0fd26475086aa03775b57e4fd",
            "473dfa87df0e40b693937f7d6d0cf226",
            "e602d814e7f84625a79bb7f8bc447a5f",
            "b41d1878fa974c9abb486dc3a79c8c14",
            "49d4dc51f4c34691b47e81f7868f7815",
            "671b9506623d43cb805de88206f8edfd",
            "89584653839e40038590321f95c88d25",
            "c26e263d193a4f60a6f955fb556c5148",
            "ad0c4788ea2c41e58f9fea9e22ba08f4",
            "afa4d598cd394eee9c96dc56f67054b8",
            "9f1919832dca44ee9fa44591a4db6b2d",
            "ade32eda99544aceab70c0a0fafb13e4",
            "a11d80627b9143c1bdf804e1302cb28c",
            "9dbfb853b4e044d0a6e4fb7c7c93f344",
            "b30516480d6346f5a78029ae396b967a",
            "e4ec4800a86d4a698498290ce57dd735",
            "e967aa72a22d4592b9b5456d698dba2d",
            "3f6735e177244c8eaa584cb6b85cc3d2",
            "6160c5f3221d42df843f925b2ed60ee8",
            "851414b2348d45a9b8d4528e2cd6c1a8",
            "c8bb8fb19b7d43aa9a31952f439d7350",
            "a06dab62469e4d808882ee25397e25e3",
            "2899a1a8a9af4c41b3b51115d6df61f7",
            "0e5d46fa954f4957b702f555de7eccc8",
            "47ed96942411497db29dcb3daddc9494",
            "ac9919ef050148d9889cf5ac80adc8ab",
            "f318018f29054af3b85e0f477c628776",
            "eeb3ba5dd2ea401f86db844c1bbd210d",
            "1ad4967acab74a768864b581b1cddbc0",
            "51a33c90016a493eb6f07e1eb7cd6162"
          ]
        },
        "id": "qXX9e6NmHMzk",
        "outputId": "0e1df5b3-6907-47a0-b9a3-9d9559c394ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/557 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d4b4205a6a84836804f0d452a22b18b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/543M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbff200216d54e1095b82b0e5f3a1691"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "408e42825bc64bae949483b1252cc8d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad0c4788ea2c41e58f9fea9e22ba08f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/895k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "851414b2348d45a9b8d4528e2cd6c1a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/vocab.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir vinai\n",
        "!chmod 777 vinai\n",
        "!mkdir vinai/phobert-base\n",
        "!chmod 777 vinai/phobert-base\n",
        "!cp /root/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/* vinai/phobert-base/."
      ],
      "metadata": {
        "id": "VS6ilClhHkEO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['__label__sống_trẻ', '__label__thời_sự', '__label__công_nghệ', '__label__sức_khỏe', '__label__giáo_dục', '__label__xe_360', '__label__thời_trang', '__label__du_lịch', '__label__âm_nhạc', '__label__xuất_bản', '__label__nhịp_sống', '__label__kinh_doanh', '__label__pháp_luật', '__label__ẩm_thực', '__label__thế_giới', '__label__thể_thao', '__label__giải_trí', '__label__phim_ảnh']\n",
        "\n",
        "train_path = 'train.txt'\n",
        "test_path = 'test.txt'\n",
        "\n",
        "MAX_LEN = 256\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", local_files_only=True)\n",
        "train_dataloader = dataloader_from_text(train_path, \n",
        "                                        tokenizer=tokenizer, \n",
        "                                        classes=classes, \n",
        "                                        savetodisk=None, \n",
        "                                        max_len=MAX_LEN, \n",
        "                                        batch_size=32)\n",
        "valid_dataloader = dataloader_from_text(test_path, \n",
        "                                        tokenizer=tokenizer, \n",
        "                                        classes=classes, \n",
        "                                        savetodisk=None, \n",
        "                                        max_len=MAX_LEN, \n",
        "                                        batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLJlMVOCWMMT",
        "outputId": "4d410e6f-b4d1-40d0-899b-dfe6041d6dfe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADDING TEXT FILE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8054it [00:00, 30538.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT TO IDS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/8054 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 256). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 8054/8054 [00:19<00:00, 411.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONVERT TO TORCH TENSOR\n",
            "CREATE DATALOADER\n",
            "len dataloader: 252\n",
            "LOAD DATA ALL DONE\n",
            "LOADDING TEXT FILE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1281it [00:00, 50196.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT TO IDS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1281/1281 [00:02<00:00, 462.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONVERT TO TORCH TENSOR\n",
            "CREATE DATALOADER\n",
            "len dataloader: 41\n",
            "LOAD DATA ALL DONE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert model\n",
        "bert_classifier_model = BERTClassifier(len(classes))\n",
        "# Initialize model\n",
        "bert_classifier_trainer = ClassifierTrainner(bert_model=bert_classifier_model, \n",
        "                                             train_dataloader=train_dataloader, \n",
        "                                             valid_dataloader=valid_dataloader, \n",
        "                                             epochs=5, \n",
        "                                             cuda_device=\"0\") #cuda_device: \"cpu\"=cpu hoac 0=gpu0, 1=gpu1, "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwdukqxaYloT",
        "outputId": "1b1b6792-d92e-423f-e931-ab3957fcaef6",
        "collapsed": true
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOAD BERT PRETRAIN MODEL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./vinai/phobert-base/pytorch_model.bin were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./vinai/phobert-base/pytorch_model.bin and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training new model, save to: 03-10-2022_07-37-24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model \n",
        "accur_train, loss_train, accur_val, loss_val = bert_classifier_trainer.train_classifier()\n",
        "\n",
        "# Plot model\n",
        "bert_classifier_trainer.plot_model(accur_train, loss_train, accur_val, loss_val)"
      ],
      "metadata": {
        "id": "orb-sBhSjgKC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4fd2228-70e8-4b0b-d583-3ff990e4cf6e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "[TRAIN] Epoch 1/5 | Batch 0/252 | Train Loss=2.8872456550598145 | Train Acc=0.0625\n",
            "[TRAIN] Epoch 1/5 | Batch 1/252 | Train Loss=2.880887985229492 | Train Acc=0.0625\n",
            "[TRAIN] Epoch 1/5 | Batch 2/252 | Train Loss=2.908358573913574 | Train Acc=0.0625\n",
            "[TRAIN] Epoch 1/5 | Batch 3/252 | Train Loss=2.8921031951904297 | Train Acc=0.125\n",
            "[TRAIN] Epoch 1/5 | Batch 4/252 | Train Loss=2.9368934631347656 | Train Acc=0.03125\n",
            "[TRAIN] Epoch 1/5 | Batch 5/252 | Train Loss=2.8807904720306396 | Train Acc=0.125\n",
            "[TRAIN] Epoch 1/5 | Batch 6/252 | Train Loss=2.8094496726989746 | Train Acc=0.21875\n",
            "[TRAIN] Epoch 1/5 | Batch 7/252 | Train Loss=2.8441741466522217 | Train Acc=0.0625\n",
            "[TRAIN] Epoch 1/5 | Batch 8/252 | Train Loss=2.791451930999756 | Train Acc=0.15625\n",
            "[TRAIN] Epoch 1/5 | Batch 9/252 | Train Loss=2.811542510986328 | Train Acc=0.1875\n",
            "[TRAIN] Epoch 1/5 | Batch 10/252 | Train Loss=2.750427484512329 | Train Acc=0.28125\n",
            "[TRAIN] Epoch 1/5 | Batch 11/252 | Train Loss=2.7986879348754883 | Train Acc=0.09375\n",
            "[TRAIN] Epoch 1/5 | Batch 12/252 | Train Loss=2.71736216545105 | Train Acc=0.3125\n",
            "[TRAIN] Epoch 1/5 | Batch 13/252 | Train Loss=2.5763823986053467 | Train Acc=0.375\n",
            "[TRAIN] Epoch 1/5 | Batch 14/252 | Train Loss=2.6390433311462402 | Train Acc=0.40625\n",
            "[TRAIN] Epoch 1/5 | Batch 15/252 | Train Loss=2.651613473892212 | Train Acc=0.28125\n",
            "[TRAIN] Epoch 1/5 | Batch 16/252 | Train Loss=2.633288621902466 | Train Acc=0.34375\n",
            "[TRAIN] Epoch 1/5 | Batch 17/252 | Train Loss=2.720280647277832 | Train Acc=0.25\n",
            "[TRAIN] Epoch 1/5 | Batch 18/252 | Train Loss=2.637532949447632 | Train Acc=0.40625\n",
            "[TRAIN] Epoch 1/5 | Batch 19/252 | Train Loss=2.614060163497925 | Train Acc=0.46875\n",
            "[TRAIN] Epoch 1/5 | Batch 20/252 | Train Loss=2.678971767425537 | Train Acc=0.28125\n",
            "[TRAIN] Epoch 1/5 | Batch 21/252 | Train Loss=2.5455758571624756 | Train Acc=0.46875\n",
            "[TRAIN] Epoch 1/5 | Batch 22/252 | Train Loss=2.4914889335632324 | Train Acc=0.65625\n",
            "[TRAIN] Epoch 1/5 | Batch 23/252 | Train Loss=2.5906171798706055 | Train Acc=0.46875\n",
            "[TRAIN] Epoch 1/5 | Batch 24/252 | Train Loss=2.5708210468292236 | Train Acc=0.40625\n",
            "[TRAIN] Epoch 1/5 | Batch 25/252 | Train Loss=2.499964952468872 | Train Acc=0.4375\n",
            "[TRAIN] Epoch 1/5 | Batch 26/252 | Train Loss=2.513042688369751 | Train Acc=0.4375\n",
            "[TRAIN] Epoch 1/5 | Batch 27/252 | Train Loss=2.3653366565704346 | Train Acc=0.5625\n",
            "[TRAIN] Epoch 1/5 | Batch 28/252 | Train Loss=2.4088165760040283 | Train Acc=0.4375\n",
            "[TRAIN] Epoch 1/5 | Batch 29/252 | Train Loss=2.3678030967712402 | Train Acc=0.5625\n",
            "[TRAIN] Epoch 1/5 | Batch 30/252 | Train Loss=2.3671317100524902 | Train Acc=0.625\n",
            "[TRAIN] Epoch 1/5 | Batch 31/252 | Train Loss=2.310438871383667 | Train Acc=0.40625\n",
            "[TRAIN] Epoch 1/5 | Batch 32/252 | Train Loss=2.27205753326416 | Train Acc=0.59375\n",
            "[TRAIN] Epoch 1/5 | Batch 33/252 | Train Loss=2.2974867820739746 | Train Acc=0.5625\n",
            "[TRAIN] Epoch 1/5 | Batch 34/252 | Train Loss=2.2804131507873535 | Train Acc=0.65625\n",
            "[TRAIN] Epoch 1/5 | Batch 35/252 | Train Loss=2.2547664642333984 | Train Acc=0.59375\n",
            "[TRAIN] Epoch 1/5 | Batch 36/252 | Train Loss=2.180824041366577 | Train Acc=0.5625\n",
            "[TRAIN] Epoch 1/5 | Batch 37/252 | Train Loss=2.320157051086426 | Train Acc=0.53125\n",
            "[TRAIN] Epoch 1/5 | Batch 38/252 | Train Loss=2.1445975303649902 | Train Acc=0.65625\n",
            "[TRAIN] Epoch 1/5 | Batch 39/252 | Train Loss=2.2159652709960938 | Train Acc=0.53125\n",
            "[TRAIN] Epoch 1/5 | Batch 40/252 | Train Loss=2.058346748352051 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 41/252 | Train Loss=2.1719515323638916 | Train Acc=0.5625\n",
            "[TRAIN] Epoch 1/5 | Batch 42/252 | Train Loss=1.9467768669128418 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 43/252 | Train Loss=2.1021528244018555 | Train Acc=0.53125\n",
            "[TRAIN] Epoch 1/5 | Batch 44/252 | Train Loss=1.9693570137023926 | Train Acc=0.65625\n",
            "[TRAIN] Epoch 1/5 | Batch 45/252 | Train Loss=1.910781741142273 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 46/252 | Train Loss=2.0099411010742188 | Train Acc=0.625\n",
            "[TRAIN] Epoch 1/5 | Batch 47/252 | Train Loss=1.8370102643966675 | Train Acc=0.75\n",
            "[TRAIN] Epoch 1/5 | Batch 48/252 | Train Loss=1.9392753839492798 | Train Acc=0.59375\n",
            "[TRAIN] Epoch 1/5 | Batch 49/252 | Train Loss=2.1066083908081055 | Train Acc=0.65625\n",
            "[TRAIN] Epoch 1/5 | Batch 50/252 | Train Loss=1.9170329570770264 | Train Acc=0.65625\n",
            "[TRAIN] Epoch 1/5 | Batch 51/252 | Train Loss=1.8243311643600464 | Train Acc=0.84375\n",
            "[TRAIN] Epoch 1/5 | Batch 52/252 | Train Loss=1.922514796257019 | Train Acc=0.59375\n",
            "[TRAIN] Epoch 1/5 | Batch 53/252 | Train Loss=1.7953152656555176 | Train Acc=0.75\n",
            "[TRAIN] Epoch 1/5 | Batch 54/252 | Train Loss=1.8451569080352783 | Train Acc=0.625\n",
            "[TRAIN] Epoch 1/5 | Batch 55/252 | Train Loss=1.6520811319351196 | Train Acc=0.78125\n",
            "[TRAIN] Epoch 1/5 | Batch 56/252 | Train Loss=1.6331199407577515 | Train Acc=0.75\n",
            "[TRAIN] Epoch 1/5 | Batch 57/252 | Train Loss=1.7222753763198853 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 58/252 | Train Loss=1.723689079284668 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 59/252 | Train Loss=1.7482001781463623 | Train Acc=0.75\n",
            "[TRAIN] Epoch 1/5 | Batch 60/252 | Train Loss=1.7760225534439087 | Train Acc=0.65625\n",
            "[TRAIN] Epoch 1/5 | Batch 61/252 | Train Loss=1.870017170906067 | Train Acc=0.5625\n",
            "[TRAIN] Epoch 1/5 | Batch 62/252 | Train Loss=1.7352076768875122 | Train Acc=0.625\n",
            "[TRAIN] Epoch 1/5 | Batch 63/252 | Train Loss=1.6996339559555054 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 64/252 | Train Loss=1.595174789428711 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 65/252 | Train Loss=1.5221010446548462 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 66/252 | Train Loss=1.3490653038024902 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 67/252 | Train Loss=1.591328501701355 | Train Acc=0.6875\n",
            "[TRAIN] Epoch 1/5 | Batch 68/252 | Train Loss=1.6028177738189697 | Train Acc=0.6875\n",
            "[TRAIN] Epoch 1/5 | Batch 69/252 | Train Loss=1.586751103401184 | Train Acc=0.75\n",
            "[TRAIN] Epoch 1/5 | Batch 70/252 | Train Loss=1.813585638999939 | Train Acc=0.625\n",
            "[TRAIN] Epoch 1/5 | Batch 71/252 | Train Loss=1.38925039768219 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 72/252 | Train Loss=1.3538075685501099 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 73/252 | Train Loss=1.4705069065093994 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 74/252 | Train Loss=1.3219153881072998 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 75/252 | Train Loss=1.3930416107177734 | Train Acc=0.78125\n",
            "[TRAIN] Epoch 1/5 | Batch 76/252 | Train Loss=1.3657374382019043 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 77/252 | Train Loss=1.2122609615325928 | Train Acc=0.78125\n",
            "[TRAIN] Epoch 1/5 | Batch 78/252 | Train Loss=1.1415653228759766 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 79/252 | Train Loss=1.4409599304199219 | Train Acc=0.65625\n",
            "[TRAIN] Epoch 1/5 | Batch 80/252 | Train Loss=1.4353933334350586 | Train Acc=0.6875\n",
            "[TRAIN] Epoch 1/5 | Batch 81/252 | Train Loss=1.422410249710083 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 82/252 | Train Loss=1.1992367506027222 | Train Acc=0.84375\n",
            "[TRAIN] Epoch 1/5 | Batch 83/252 | Train Loss=1.2659317255020142 | Train Acc=0.78125\n",
            "[TRAIN] Epoch 1/5 | Batch 84/252 | Train Loss=1.3468799591064453 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 85/252 | Train Loss=1.2635033130645752 | Train Acc=0.75\n",
            "[TRAIN] Epoch 1/5 | Batch 86/252 | Train Loss=1.3494929075241089 | Train Acc=0.75\n",
            "[TRAIN] Epoch 1/5 | Batch 87/252 | Train Loss=1.29195237159729 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 88/252 | Train Loss=1.2547422647476196 | Train Acc=0.75\n",
            "[TRAIN] Epoch 1/5 | Batch 89/252 | Train Loss=1.1861640214920044 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 90/252 | Train Loss=1.2870005369186401 | Train Acc=0.75\n",
            "[TRAIN] Epoch 1/5 | Batch 91/252 | Train Loss=1.2569290399551392 | Train Acc=0.84375\n",
            "[TRAIN] Epoch 1/5 | Batch 92/252 | Train Loss=1.0515201091766357 | Train Acc=0.84375\n",
            "[TRAIN] Epoch 1/5 | Batch 93/252 | Train Loss=1.3441115617752075 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 94/252 | Train Loss=1.060410976409912 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 95/252 | Train Loss=1.085455060005188 | Train Acc=0.84375\n",
            "[TRAIN] Epoch 1/5 | Batch 96/252 | Train Loss=1.103900671005249 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 97/252 | Train Loss=1.2112078666687012 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 98/252 | Train Loss=1.3374332189559937 | Train Acc=0.6875\n",
            "[TRAIN] Epoch 1/5 | Batch 99/252 | Train Loss=1.0925710201263428 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 100/252 | Train Loss=1.4052220582962036 | Train Acc=0.6875\n",
            "Running Validation...\n",
            " Valid Loss: 1.0012\n",
            " Valid Accuracy: 0.8110\n",
            " Valid F1 score: 0.7238\n",
            "best_valid_loss = 999999.0000\n",
            "eval_loss = 1.0012\n",
            "best_eval_accuracy = 0.0000\n",
            "eval_accuracy = 0.8110\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valacc5epochs.pth\n",
            " Train Accuracy: 0.5916\n",
            " Train F1 score: 0.5056\n",
            " Train Loss: 0.7832\n",
            "[TRAIN] Epoch 1/5 | Batch 101/252 | Train Loss=1.0808080434799194 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 102/252 | Train Loss=1.0881611108779907 | Train Acc=0.78125\n",
            "[TRAIN] Epoch 1/5 | Batch 103/252 | Train Loss=0.7908564209938049 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 104/252 | Train Loss=1.1690157651901245 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 105/252 | Train Loss=0.9663903713226318 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 106/252 | Train Loss=0.8601627945899963 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 107/252 | Train Loss=1.0769304037094116 | Train Acc=0.78125\n",
            "[TRAIN] Epoch 1/5 | Batch 108/252 | Train Loss=1.1401007175445557 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 109/252 | Train Loss=0.7719590067863464 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 110/252 | Train Loss=0.8810325860977173 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 111/252 | Train Loss=0.9985904097557068 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 112/252 | Train Loss=0.7571817636489868 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 113/252 | Train Loss=1.0889724493026733 | Train Acc=0.71875\n",
            "[TRAIN] Epoch 1/5 | Batch 114/252 | Train Loss=0.9626056551933289 | Train Acc=0.78125\n",
            "[TRAIN] Epoch 1/5 | Batch 115/252 | Train Loss=0.9032353758811951 | Train Acc=0.8125\n",
            "[TRAIN] Epoch 1/5 | Batch 116/252 | Train Loss=0.7046618461608887 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 117/252 | Train Loss=0.6432884335517883 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 118/252 | Train Loss=0.7451828122138977 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 119/252 | Train Loss=0.6628987193107605 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 120/252 | Train Loss=0.663483738899231 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 121/252 | Train Loss=0.6636257767677307 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 122/252 | Train Loss=0.8080875873565674 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 123/252 | Train Loss=0.8083301186561584 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 124/252 | Train Loss=0.7100025415420532 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 125/252 | Train Loss=0.7261164784431458 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 126/252 | Train Loss=0.6162552833557129 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 127/252 | Train Loss=0.6609578728675842 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 128/252 | Train Loss=0.7552792429924011 | Train Acc=0.84375\n",
            "[TRAIN] Epoch 1/5 | Batch 129/252 | Train Loss=0.6020116806030273 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 130/252 | Train Loss=0.6544392704963684 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 131/252 | Train Loss=0.6344888806343079 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 132/252 | Train Loss=0.5722872614860535 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 133/252 | Train Loss=0.6446279287338257 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 134/252 | Train Loss=0.7192486524581909 | Train Acc=0.84375\n",
            "[TRAIN] Epoch 1/5 | Batch 135/252 | Train Loss=0.47599390149116516 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 136/252 | Train Loss=0.6174879670143127 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 137/252 | Train Loss=0.6016020774841309 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 138/252 | Train Loss=0.5976099371910095 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 139/252 | Train Loss=0.5817549228668213 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 140/252 | Train Loss=0.46265560388565063 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 141/252 | Train Loss=0.5783759951591492 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 142/252 | Train Loss=0.67899090051651 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 143/252 | Train Loss=0.5407418012619019 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 144/252 | Train Loss=0.5822293758392334 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 145/252 | Train Loss=0.4931200444698334 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 146/252 | Train Loss=0.398468017578125 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 147/252 | Train Loss=0.4102952480316162 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 148/252 | Train Loss=0.39501264691352844 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 149/252 | Train Loss=0.6625956296920776 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 150/252 | Train Loss=0.5232952833175659 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 151/252 | Train Loss=0.41675782203674316 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 152/252 | Train Loss=0.3607611358165741 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 153/252 | Train Loss=0.4434225857257843 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 154/252 | Train Loss=0.33216434717178345 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 155/252 | Train Loss=0.40182965993881226 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 156/252 | Train Loss=0.43785202503204346 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 157/252 | Train Loss=0.4718768298625946 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 158/252 | Train Loss=0.5016167759895325 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 159/252 | Train Loss=0.4665650427341461 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 160/252 | Train Loss=0.3051738440990448 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 161/252 | Train Loss=0.3689538240432739 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 162/252 | Train Loss=0.4336080551147461 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 163/252 | Train Loss=0.5201091766357422 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 164/252 | Train Loss=0.3657970428466797 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 165/252 | Train Loss=0.3073514699935913 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 166/252 | Train Loss=0.3076688051223755 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 167/252 | Train Loss=0.5476467609405518 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 168/252 | Train Loss=0.30710941553115845 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 169/252 | Train Loss=0.44070664048194885 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 170/252 | Train Loss=0.46550077199935913 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 171/252 | Train Loss=0.3841552436351776 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 172/252 | Train Loss=0.2597120404243469 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 173/252 | Train Loss=0.32731616497039795 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 174/252 | Train Loss=0.27312585711479187 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 175/252 | Train Loss=0.3161254823207855 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 176/252 | Train Loss=0.251761794090271 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 177/252 | Train Loss=0.31486159563064575 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 178/252 | Train Loss=0.33206358551979065 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 179/252 | Train Loss=0.2220277488231659 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 180/252 | Train Loss=0.24722617864608765 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 181/252 | Train Loss=0.21680571138858795 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 182/252 | Train Loss=0.3271622657775879 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 183/252 | Train Loss=0.3925729990005493 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 184/252 | Train Loss=0.2869546413421631 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 185/252 | Train Loss=0.2648065388202667 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 186/252 | Train Loss=0.22426089644432068 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 187/252 | Train Loss=0.2676495313644409 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 188/252 | Train Loss=0.2936970591545105 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 189/252 | Train Loss=0.25006747245788574 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 190/252 | Train Loss=0.23090782761573792 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 191/252 | Train Loss=0.37947753071784973 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 192/252 | Train Loss=0.21981270611286163 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 193/252 | Train Loss=0.3281843364238739 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 194/252 | Train Loss=0.353380411863327 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 195/252 | Train Loss=0.33459794521331787 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 196/252 | Train Loss=0.2539651095867157 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 197/252 | Train Loss=0.21467137336730957 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 198/252 | Train Loss=0.21360376477241516 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 199/252 | Train Loss=0.19233910739421844 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 200/252 | Train Loss=0.21852798759937286 | Train Acc=1.0\n",
            "Running Validation...\n",
            " Valid Loss: 0.2275\n",
            " Valid Accuracy: 0.9825\n",
            " Valid F1 score: 0.9715\n",
            "best_valid_loss = 1.0012\n",
            "eval_loss = 0.2275\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valoss_5epochs.pth\n",
            "best_eval_accuracy = 0.8110\n",
            "eval_accuracy = 0.9825\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valacc5epochs.pth\n",
            " Train Accuracy: 0.7617\n",
            " Train F1 score: 0.7056\n",
            " Train Loss: 0.9925\n",
            "[TRAIN] Epoch 1/5 | Batch 201/252 | Train Loss=0.18798957765102386 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 202/252 | Train Loss=0.30422934889793396 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 203/252 | Train Loss=0.17804931104183197 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 204/252 | Train Loss=0.18520988523960114 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 205/252 | Train Loss=0.3079669177532196 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 206/252 | Train Loss=0.2436559796333313 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 207/252 | Train Loss=0.24116604030132294 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 208/252 | Train Loss=0.17244577407836914 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 209/252 | Train Loss=0.2148592174053192 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 210/252 | Train Loss=0.33521124720573425 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 1/5 | Batch 211/252 | Train Loss=0.26336076855659485 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 212/252 | Train Loss=0.16157254576683044 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 213/252 | Train Loss=0.1800924390554428 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 214/252 | Train Loss=0.19128400087356567 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 215/252 | Train Loss=0.17346107959747314 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 216/252 | Train Loss=0.19567237794399261 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 217/252 | Train Loss=0.2459930032491684 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 218/252 | Train Loss=0.1667018085718155 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 219/252 | Train Loss=0.1459716558456421 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 220/252 | Train Loss=0.18379126489162445 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 221/252 | Train Loss=0.22050316631793976 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 222/252 | Train Loss=0.13995102047920227 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 223/252 | Train Loss=0.1406780183315277 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 224/252 | Train Loss=0.14051786065101624 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 225/252 | Train Loss=0.16832679510116577 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 226/252 | Train Loss=0.13129283487796783 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 227/252 | Train Loss=0.2106723189353943 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 228/252 | Train Loss=0.13283231854438782 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 229/252 | Train Loss=0.17424562573432922 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 230/252 | Train Loss=0.13810135424137115 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 231/252 | Train Loss=0.13170549273490906 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 232/252 | Train Loss=0.12149600684642792 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 233/252 | Train Loss=0.11376252770423889 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 234/252 | Train Loss=0.24452969431877136 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 235/252 | Train Loss=0.3569191098213196 | Train Acc=0.875\n",
            "[TRAIN] Epoch 1/5 | Batch 236/252 | Train Loss=0.2851247191429138 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 237/252 | Train Loss=0.11116999387741089 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 238/252 | Train Loss=0.20220015943050385 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 239/252 | Train Loss=0.4029310643672943 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 1/5 | Batch 240/252 | Train Loss=0.1502523571252823 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 241/252 | Train Loss=0.13328365981578827 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 242/252 | Train Loss=0.11064456403255463 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 243/252 | Train Loss=0.17529603838920593 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 244/252 | Train Loss=0.14824576675891876 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 245/252 | Train Loss=0.1490536630153656 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 246/252 | Train Loss=0.1396551877260208 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 247/252 | Train Loss=0.13485167920589447 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 248/252 | Train Loss=0.10299189388751984 | Train Acc=1.0\n",
            "[TRAIN] Epoch 1/5 | Batch 249/252 | Train Loss=0.15136824548244476 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 250/252 | Train Loss=0.16708576679229736 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 1/5 | Batch 251/252 | Train Loss=0.10612168163061142 | Train Acc=1.0\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "[TRAIN] Epoch 2/5 | Batch 0/252 | Train Loss=0.46181994676589966 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 2/5 | Batch 1/252 | Train Loss=0.27615830302238464 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 2/252 | Train Loss=0.2429649531841278 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 3/252 | Train Loss=0.1783548891544342 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 4/252 | Train Loss=0.24348460137844086 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 5/252 | Train Loss=0.1579219102859497 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 6/252 | Train Loss=0.3205866813659668 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 2/5 | Batch 7/252 | Train Loss=0.19096574187278748 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 8/252 | Train Loss=0.3190053403377533 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 9/252 | Train Loss=0.23441225290298462 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 10/252 | Train Loss=0.1813313513994217 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 11/252 | Train Loss=0.18946406245231628 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 12/252 | Train Loss=0.17319729924201965 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 13/252 | Train Loss=0.13365864753723145 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 14/252 | Train Loss=0.19351400434970856 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 15/252 | Train Loss=0.13380610942840576 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 16/252 | Train Loss=0.19252647459506989 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 17/252 | Train Loss=0.21190039813518524 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 18/252 | Train Loss=0.13169805705547333 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 19/252 | Train Loss=0.13742753863334656 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 20/252 | Train Loss=0.23665021359920502 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 21/252 | Train Loss=0.2341819554567337 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 2/5 | Batch 22/252 | Train Loss=0.12562912702560425 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 23/252 | Train Loss=0.11896573007106781 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 24/252 | Train Loss=0.2703617215156555 | Train Acc=0.90625\n",
            "[TRAIN] Epoch 2/5 | Batch 25/252 | Train Loss=0.12429903447628021 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 26/252 | Train Loss=0.16281680762767792 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 27/252 | Train Loss=0.13909956812858582 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 28/252 | Train Loss=0.19089490175247192 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 29/252 | Train Loss=0.16794458031654358 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 30/252 | Train Loss=0.13304761052131653 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 31/252 | Train Loss=0.11470431089401245 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 32/252 | Train Loss=0.13099633157253265 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 33/252 | Train Loss=0.11889375001192093 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 34/252 | Train Loss=0.11448123306035995 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 35/252 | Train Loss=0.11782100051641464 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 36/252 | Train Loss=0.24086126685142517 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 37/252 | Train Loss=0.16826775670051575 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 38/252 | Train Loss=0.12237443029880524 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 39/252 | Train Loss=0.10450804978609085 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 40/252 | Train Loss=0.17002703249454498 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 41/252 | Train Loss=0.2178383469581604 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 42/252 | Train Loss=0.16080264747142792 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 43/252 | Train Loss=0.10808917880058289 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 44/252 | Train Loss=0.1115529015660286 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 45/252 | Train Loss=0.10752415657043457 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 46/252 | Train Loss=0.10068190097808838 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 47/252 | Train Loss=0.2768506407737732 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 2/5 | Batch 48/252 | Train Loss=0.2677573263645172 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 49/252 | Train Loss=0.16826766729354858 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 50/252 | Train Loss=0.09859117865562439 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 51/252 | Train Loss=0.11609642952680588 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 52/252 | Train Loss=0.11084314435720444 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 53/252 | Train Loss=0.15613657236099243 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 54/252 | Train Loss=0.08942590653896332 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 55/252 | Train Loss=0.08973037451505661 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 56/252 | Train Loss=0.3281494975090027 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 2/5 | Batch 57/252 | Train Loss=0.09956634044647217 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 58/252 | Train Loss=0.0936087816953659 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 59/252 | Train Loss=0.2303372621536255 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 60/252 | Train Loss=0.11959586292505264 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 61/252 | Train Loss=0.38866978883743286 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 2/5 | Batch 62/252 | Train Loss=0.10846222192049026 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 63/252 | Train Loss=0.1896234154701233 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 64/252 | Train Loss=0.17102932929992676 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 65/252 | Train Loss=0.08294694870710373 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 66/252 | Train Loss=0.08819539099931717 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 67/252 | Train Loss=0.0977303609251976 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 68/252 | Train Loss=0.11109314858913422 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 69/252 | Train Loss=0.29555341601371765 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 2/5 | Batch 70/252 | Train Loss=0.11491508781909943 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 71/252 | Train Loss=0.09756142646074295 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 72/252 | Train Loss=0.08077843487262726 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 73/252 | Train Loss=0.09063159674406052 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 74/252 | Train Loss=0.07982853055000305 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 75/252 | Train Loss=0.07845797389745712 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 76/252 | Train Loss=0.1326962113380432 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 77/252 | Train Loss=0.09013164788484573 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 78/252 | Train Loss=0.07360760867595673 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 79/252 | Train Loss=0.07689689099788666 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 80/252 | Train Loss=0.07937192171812057 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 81/252 | Train Loss=0.0797777771949768 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 82/252 | Train Loss=0.08310554176568985 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 83/252 | Train Loss=0.07126575708389282 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 84/252 | Train Loss=0.22568689286708832 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 2/5 | Batch 85/252 | Train Loss=0.1010872945189476 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 86/252 | Train Loss=0.07159393280744553 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 87/252 | Train Loss=0.06586837023496628 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 88/252 | Train Loss=0.06942033767700195 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 89/252 | Train Loss=0.06962189823389053 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 90/252 | Train Loss=0.2625422477722168 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 2/5 | Batch 91/252 | Train Loss=0.16347169876098633 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 92/252 | Train Loss=0.09103228151798248 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 93/252 | Train Loss=0.15678183734416962 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 94/252 | Train Loss=0.0682663545012474 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 95/252 | Train Loss=0.1908148229122162 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 96/252 | Train Loss=0.06625647842884064 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 97/252 | Train Loss=0.06481495499610901 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 98/252 | Train Loss=0.1940331608057022 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 99/252 | Train Loss=0.09252137690782547 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 100/252 | Train Loss=0.092166468501091 | Train Acc=1.0\n",
            "Running Validation...\n",
            " Valid Loss: 0.0584\n",
            " Valid Accuracy: 0.9954\n",
            " Valid F1 score: 0.9927\n",
            "best_valid_loss = 0.2275\n",
            "eval_loss = 0.0584\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valoss_5epochs.pth\n",
            "best_eval_accuracy = 0.9825\n",
            "eval_accuracy = 0.9954\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valacc5epochs.pth\n",
            " Train Accuracy: 0.9842\n",
            " Train F1 score: 0.9794\n",
            " Train Loss: 0.0614\n",
            "[TRAIN] Epoch 2/5 | Batch 101/252 | Train Loss=0.041364096105098724 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 102/252 | Train Loss=0.044674620032310486 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 103/252 | Train Loss=0.04040367156267166 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 104/252 | Train Loss=0.2069997787475586 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 105/252 | Train Loss=0.045614615082740784 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 106/252 | Train Loss=0.043307941406965256 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 107/252 | Train Loss=0.04340926930308342 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 108/252 | Train Loss=0.039366889744997025 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 109/252 | Train Loss=0.04685794934630394 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 110/252 | Train Loss=0.04308416321873665 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 111/252 | Train Loss=0.14093759655952454 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 112/252 | Train Loss=0.14974413812160492 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 113/252 | Train Loss=0.03916192799806595 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 114/252 | Train Loss=0.041731998324394226 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 115/252 | Train Loss=0.09149030596017838 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 116/252 | Train Loss=0.14021648466587067 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 117/252 | Train Loss=0.03660379350185394 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 118/252 | Train Loss=0.037928756326436996 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 119/252 | Train Loss=0.04208939149975777 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 120/252 | Train Loss=0.03805536404252052 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 121/252 | Train Loss=0.036325663328170776 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 122/252 | Train Loss=0.037754204124212265 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 123/252 | Train Loss=0.037691131234169006 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 124/252 | Train Loss=0.04721058905124664 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 125/252 | Train Loss=0.035147037357091904 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 126/252 | Train Loss=0.03480390086770058 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 127/252 | Train Loss=0.1666734665632248 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 128/252 | Train Loss=0.0362163707613945 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 129/252 | Train Loss=0.034325145184993744 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 130/252 | Train Loss=0.06297329068183899 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 131/252 | Train Loss=0.03597910702228546 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 132/252 | Train Loss=0.03388134762644768 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 133/252 | Train Loss=0.03309550881385803 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 134/252 | Train Loss=0.1636914163827896 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 135/252 | Train Loss=0.03311176970601082 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 136/252 | Train Loss=0.03354911878705025 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 137/252 | Train Loss=0.034179285168647766 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 138/252 | Train Loss=0.03354135900735855 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 139/252 | Train Loss=0.03207690641283989 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 140/252 | Train Loss=0.032717473804950714 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 141/252 | Train Loss=0.031888511031866074 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 142/252 | Train Loss=0.034794438630342484 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 143/252 | Train Loss=0.057292234152555466 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 144/252 | Train Loss=0.035758908838033676 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 145/252 | Train Loss=0.030967961996793747 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 146/252 | Train Loss=0.03063616342842579 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 147/252 | Train Loss=0.03171953186392784 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 148/252 | Train Loss=0.030701151117682457 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 149/252 | Train Loss=0.04650254547595978 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 150/252 | Train Loss=0.03001701831817627 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 151/252 | Train Loss=0.02948361448943615 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 152/252 | Train Loss=0.02848130278289318 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 153/252 | Train Loss=0.02984563447535038 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 154/252 | Train Loss=0.029052400961518288 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 155/252 | Train Loss=0.029657253995537758 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 156/252 | Train Loss=0.03035806119441986 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 157/252 | Train Loss=0.028910785913467407 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 158/252 | Train Loss=0.03321080282330513 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 159/252 | Train Loss=0.029095783829689026 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 160/252 | Train Loss=0.027469830587506294 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 161/252 | Train Loss=0.028656968846917152 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 162/252 | Train Loss=0.029359493404626846 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 163/252 | Train Loss=0.19897180795669556 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 164/252 | Train Loss=0.026943868026137352 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 165/252 | Train Loss=0.027770066633820534 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 166/252 | Train Loss=0.027694493532180786 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 167/252 | Train Loss=0.028319334611296654 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 168/252 | Train Loss=0.02685670368373394 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 169/252 | Train Loss=0.029650768265128136 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 170/252 | Train Loss=0.028676412999629974 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 171/252 | Train Loss=0.027796298265457153 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 172/252 | Train Loss=0.02576509863138199 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 173/252 | Train Loss=0.026388896629214287 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 174/252 | Train Loss=0.026157692074775696 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 175/252 | Train Loss=0.0271635539829731 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 176/252 | Train Loss=0.028760433197021484 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 177/252 | Train Loss=0.02773580141365528 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 178/252 | Train Loss=0.025846634060144424 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 179/252 | Train Loss=0.025483189150691032 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 180/252 | Train Loss=0.025392884388566017 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 181/252 | Train Loss=0.024263378232717514 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 182/252 | Train Loss=0.025719359517097473 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 183/252 | Train Loss=0.026240991428494453 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 184/252 | Train Loss=0.02455698885023594 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 185/252 | Train Loss=0.02590670995414257 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 186/252 | Train Loss=0.02424992434680462 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 187/252 | Train Loss=0.025490066036581993 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 188/252 | Train Loss=0.024736616760492325 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 189/252 | Train Loss=0.024375708773732185 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 190/252 | Train Loss=0.024351781234145164 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 191/252 | Train Loss=0.02480243146419525 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 192/252 | Train Loss=0.023946506902575493 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 193/252 | Train Loss=0.0260305292904377 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 194/252 | Train Loss=0.02415999211370945 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 195/252 | Train Loss=0.025621499866247177 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 196/252 | Train Loss=0.022645268589258194 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 197/252 | Train Loss=0.02299472875893116 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 198/252 | Train Loss=0.023051206022500992 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 199/252 | Train Loss=0.023459138348698616 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 200/252 | Train Loss=0.022260567173361778 | Train Acc=1.0\n",
            "Running Validation...\n",
            " Valid Loss: 0.0339\n",
            " Valid Accuracy: 0.9970\n",
            " Valid F1 score: 0.9972\n",
            "best_valid_loss = 0.0584\n",
            "eval_loss = 0.0339\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valoss_5epochs.pth\n",
            "best_eval_accuracy = 0.9954\n",
            "eval_accuracy = 0.9970\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valacc5epochs.pth\n",
            " Train Accuracy: 0.9907\n",
            " Train F1 score: 0.9880\n",
            " Train Loss: 0.0781\n",
            "[TRAIN] Epoch 2/5 | Batch 201/252 | Train Loss=0.023386865854263306 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 202/252 | Train Loss=0.03569178283214569 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 203/252 | Train Loss=0.022053187713027 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 204/252 | Train Loss=0.02187255769968033 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 205/252 | Train Loss=0.07913465052843094 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 206/252 | Train Loss=0.022802498191595078 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 207/252 | Train Loss=0.021616049110889435 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 208/252 | Train Loss=0.021821213886141777 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 209/252 | Train Loss=0.02185908704996109 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 210/252 | Train Loss=0.022711753845214844 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 211/252 | Train Loss=0.021889250725507736 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 212/252 | Train Loss=0.02215663529932499 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 213/252 | Train Loss=0.021720679476857185 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 214/252 | Train Loss=0.02201121859252453 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 215/252 | Train Loss=0.02119671180844307 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 216/252 | Train Loss=0.022727815434336662 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 217/252 | Train Loss=0.02175934612751007 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 218/252 | Train Loss=0.021740928292274475 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 219/252 | Train Loss=0.020404841750860214 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 220/252 | Train Loss=0.020442882552742958 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 221/252 | Train Loss=0.02179751917719841 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 222/252 | Train Loss=0.020711204037070274 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 223/252 | Train Loss=0.020890258252620697 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 224/252 | Train Loss=0.021519262343645096 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 225/252 | Train Loss=0.0205749049782753 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 226/252 | Train Loss=0.020052362233400345 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 227/252 | Train Loss=0.020975584164261818 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 228/252 | Train Loss=0.020075106993317604 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 229/252 | Train Loss=0.020115965977311134 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 230/252 | Train Loss=0.01978696696460247 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 231/252 | Train Loss=0.019451824948191643 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 232/252 | Train Loss=0.019108224660158157 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 233/252 | Train Loss=0.020162096247076988 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 234/252 | Train Loss=0.020119236782193184 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 235/252 | Train Loss=0.019554931670427322 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 236/252 | Train Loss=0.02094484493136406 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 237/252 | Train Loss=0.018992532044649124 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 238/252 | Train Loss=0.019237900152802467 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 239/252 | Train Loss=0.08934059739112854 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 2/5 | Batch 240/252 | Train Loss=0.020365778356790543 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 241/252 | Train Loss=0.018886413425207138 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 242/252 | Train Loss=0.018403567373752594 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 243/252 | Train Loss=0.018702935427427292 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 244/252 | Train Loss=0.01892324723303318 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 245/252 | Train Loss=0.018482454121112823 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 246/252 | Train Loss=0.01886823959648609 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 247/252 | Train Loss=0.018667025491595268 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 248/252 | Train Loss=0.018407469615340233 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 249/252 | Train Loss=0.01875595934689045 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 250/252 | Train Loss=0.020197130739688873 | Train Acc=1.0\n",
            "[TRAIN] Epoch 2/5 | Batch 251/252 | Train Loss=0.0184895321726799 | Train Acc=1.0\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "[TRAIN] Epoch 3/5 | Batch 0/252 | Train Loss=0.03247542306780815 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 1/252 | Train Loss=0.03175419196486473 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 2/252 | Train Loss=0.033905286341905594 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 3/252 | Train Loss=0.02794511429965496 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 4/252 | Train Loss=0.029192103073000908 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 5/252 | Train Loss=0.02846210077404976 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 6/252 | Train Loss=0.028927059844136238 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 7/252 | Train Loss=0.029813852161169052 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 8/252 | Train Loss=0.12530486285686493 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 9/252 | Train Loss=0.0299226064234972 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 10/252 | Train Loss=0.027255801483988762 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 11/252 | Train Loss=0.028240453451871872 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 12/252 | Train Loss=0.03023384138941765 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 13/252 | Train Loss=0.026561807841062546 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 14/252 | Train Loss=0.02787056192755699 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 15/252 | Train Loss=0.05827059596776962 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 16/252 | Train Loss=0.028512032702565193 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 17/252 | Train Loss=0.029396800324320793 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 18/252 | Train Loss=0.027147039771080017 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 19/252 | Train Loss=0.02574511431157589 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 20/252 | Train Loss=0.033334698528051376 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 21/252 | Train Loss=0.025737836956977844 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 22/252 | Train Loss=0.02526327595114708 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 23/252 | Train Loss=0.023873064666986465 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 24/252 | Train Loss=0.030718868598341942 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 25/252 | Train Loss=0.026159051805734634 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 26/252 | Train Loss=0.02701408788561821 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 27/252 | Train Loss=0.02722901850938797 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 28/252 | Train Loss=0.02715284749865532 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 29/252 | Train Loss=0.03146671503782272 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 30/252 | Train Loss=0.08815330266952515 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 31/252 | Train Loss=0.02537347748875618 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 32/252 | Train Loss=0.02785208821296692 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 33/252 | Train Loss=0.02413264475762844 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 34/252 | Train Loss=0.028382403776049614 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 35/252 | Train Loss=0.024867655709385872 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 36/252 | Train Loss=0.023756735026836395 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 37/252 | Train Loss=0.02551989257335663 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 38/252 | Train Loss=0.027936941012740135 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 39/252 | Train Loss=0.025611914694309235 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 40/252 | Train Loss=0.029355071485042572 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 41/252 | Train Loss=0.026933150365948677 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 42/252 | Train Loss=0.023493822664022446 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 43/252 | Train Loss=0.024057229980826378 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 44/252 | Train Loss=0.028643546625971794 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 45/252 | Train Loss=0.028628667816519737 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 46/252 | Train Loss=0.023342451080679893 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 47/252 | Train Loss=0.09864586591720581 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 3/5 | Batch 48/252 | Train Loss=0.023765666410326958 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 49/252 | Train Loss=0.028818238526582718 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 50/252 | Train Loss=0.027923783287405968 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 51/252 | Train Loss=0.02323853224515915 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 52/252 | Train Loss=0.025458207353949547 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 53/252 | Train Loss=0.02356029860675335 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 54/252 | Train Loss=0.024078218266367912 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 55/252 | Train Loss=0.02495281584560871 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 56/252 | Train Loss=0.2126568704843521 | Train Acc=0.9375\n",
            "[TRAIN] Epoch 3/5 | Batch 57/252 | Train Loss=0.02432626485824585 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 58/252 | Train Loss=0.02351517416536808 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 59/252 | Train Loss=0.022770781069993973 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 60/252 | Train Loss=0.03515101224184036 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 61/252 | Train Loss=0.09154041856527328 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 62/252 | Train Loss=0.0243044663220644 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 63/252 | Train Loss=0.022870521992444992 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 64/252 | Train Loss=0.02292446792125702 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 65/252 | Train Loss=0.023314112797379494 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 66/252 | Train Loss=0.02339608408510685 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 67/252 | Train Loss=0.021846633404493332 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 68/252 | Train Loss=0.028620122000575066 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 69/252 | Train Loss=0.022054478526115417 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 70/252 | Train Loss=0.02256278693675995 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 71/252 | Train Loss=0.019337724894285202 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 72/252 | Train Loss=0.019844306632876396 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 73/252 | Train Loss=0.0203006099909544 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 74/252 | Train Loss=0.021396294236183167 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 75/252 | Train Loss=0.14139032363891602 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 76/252 | Train Loss=0.021800247952342033 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 77/252 | Train Loss=0.029652129858732224 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 78/252 | Train Loss=0.02075938880443573 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 79/252 | Train Loss=0.02026611939072609 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 80/252 | Train Loss=0.021695030853152275 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 81/252 | Train Loss=0.02047155052423477 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 82/252 | Train Loss=0.019487205892801285 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 83/252 | Train Loss=0.02051868848502636 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 84/252 | Train Loss=0.024506254121661186 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 85/252 | Train Loss=0.02462303265929222 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 86/252 | Train Loss=0.020233003422617912 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 87/252 | Train Loss=0.020561767742037773 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 88/252 | Train Loss=0.020101284608244896 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 89/252 | Train Loss=0.020802782848477364 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 90/252 | Train Loss=0.21636240184307098 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 91/252 | Train Loss=0.0457121841609478 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 92/252 | Train Loss=0.020280567929148674 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 93/252 | Train Loss=0.023219004273414612 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 94/252 | Train Loss=0.019996291026473045 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 95/252 | Train Loss=0.02130153588950634 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 96/252 | Train Loss=0.01911681517958641 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 97/252 | Train Loss=0.018332358449697495 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 98/252 | Train Loss=0.022336848080158234 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 99/252 | Train Loss=0.02055552788078785 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 100/252 | Train Loss=0.01973826438188553 | Train Acc=1.0\n",
            "Running Validation...\n",
            " Valid Loss: 0.0188\n",
            " Valid Accuracy: 0.9992\n",
            " Valid F1 score: 0.9991\n",
            "best_valid_loss = 0.0339\n",
            "eval_loss = 0.0188\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valoss_5epochs.pth\n",
            "best_eval_accuracy = 0.9970\n",
            "eval_accuracy = 0.9992\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valacc5epochs.pth\n",
            " Train Accuracy: 0.9966\n",
            " Train F1 score: 0.9958\n",
            " Train Loss: 0.0134\n",
            "[TRAIN] Epoch 3/5 | Batch 101/252 | Train Loss=0.01193763967603445 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 102/252 | Train Loss=0.012203640304505825 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 103/252 | Train Loss=0.011929397471249104 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 104/252 | Train Loss=0.0725504532456398 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 105/252 | Train Loss=0.012308526784181595 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 106/252 | Train Loss=0.01279316283762455 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 107/252 | Train Loss=0.011778484098613262 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 108/252 | Train Loss=0.01157572865486145 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 109/252 | Train Loss=0.011540467850863934 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 110/252 | Train Loss=0.01175541803240776 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 111/252 | Train Loss=0.04017585143446922 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 112/252 | Train Loss=0.01267503947019577 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 113/252 | Train Loss=0.011362518183887005 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 114/252 | Train Loss=0.011394951492547989 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 115/252 | Train Loss=0.011095274239778519 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 116/252 | Train Loss=0.011361032724380493 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 117/252 | Train Loss=0.011036122217774391 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 118/252 | Train Loss=0.011050309985876083 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 119/252 | Train Loss=0.011020743288099766 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 120/252 | Train Loss=0.010983927175402641 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 121/252 | Train Loss=0.010982871986925602 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 122/252 | Train Loss=0.010888108983635902 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 123/252 | Train Loss=0.011060918681323528 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 124/252 | Train Loss=0.011353791691362858 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 125/252 | Train Loss=0.011283418163657188 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 126/252 | Train Loss=0.011355729773640633 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 127/252 | Train Loss=0.01126142404973507 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 128/252 | Train Loss=0.010801911354064941 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 129/252 | Train Loss=0.010627401061356068 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 130/252 | Train Loss=0.011179959401488304 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 131/252 | Train Loss=0.011176023632287979 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 132/252 | Train Loss=0.010631296783685684 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 133/252 | Train Loss=0.010553116910159588 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 134/252 | Train Loss=0.01076675858348608 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 135/252 | Train Loss=0.010569137521088123 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 136/252 | Train Loss=0.010357297025620937 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 137/252 | Train Loss=0.01073545403778553 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 138/252 | Train Loss=0.010323932394385338 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 139/252 | Train Loss=0.010196286253631115 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 140/252 | Train Loss=0.01040472649037838 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 141/252 | Train Loss=0.010213248431682587 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 142/252 | Train Loss=0.010448080487549305 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 143/252 | Train Loss=0.010264267213642597 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 144/252 | Train Loss=0.01031856145709753 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 145/252 | Train Loss=0.010232873260974884 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 146/252 | Train Loss=0.010009302757680416 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 147/252 | Train Loss=0.010264559648931026 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 148/252 | Train Loss=0.0100875124335289 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 149/252 | Train Loss=0.010486040264368057 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 150/252 | Train Loss=0.009981289505958557 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 151/252 | Train Loss=0.009863822720944881 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 152/252 | Train Loss=0.009778947569429874 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 153/252 | Train Loss=0.009971321560442448 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 154/252 | Train Loss=0.009790081530809402 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 155/252 | Train Loss=0.009783242829144001 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 156/252 | Train Loss=0.010157698765397072 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 157/252 | Train Loss=0.009826252236962318 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 158/252 | Train Loss=0.00999730359762907 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 159/252 | Train Loss=0.009907260537147522 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 160/252 | Train Loss=0.009592894464731216 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 161/252 | Train Loss=0.009953700937330723 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 162/252 | Train Loss=0.010009942576289177 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 163/252 | Train Loss=0.2236662358045578 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 164/252 | Train Loss=0.0093486113473773 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 165/252 | Train Loss=0.01077781431376934 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 166/252 | Train Loss=0.00957342516630888 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 167/252 | Train Loss=0.009714405052363873 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 168/252 | Train Loss=0.009553919546306133 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 169/252 | Train Loss=0.009662272408604622 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 170/252 | Train Loss=0.009733094833791256 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 171/252 | Train Loss=0.009819881059229374 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 172/252 | Train Loss=0.00952482596039772 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 173/252 | Train Loss=0.009573048911988735 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 174/252 | Train Loss=0.009455379098653793 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 175/252 | Train Loss=0.009831428527832031 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 176/252 | Train Loss=0.0099312299862504 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 177/252 | Train Loss=0.009778715670108795 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 178/252 | Train Loss=0.009559033438563347 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 179/252 | Train Loss=0.00940441619604826 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 180/252 | Train Loss=0.009299869649112225 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 181/252 | Train Loss=0.00918278656899929 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 182/252 | Train Loss=0.00934561062604189 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 183/252 | Train Loss=0.009498612023890018 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 184/252 | Train Loss=0.009182834066450596 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 185/252 | Train Loss=0.009552452713251114 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 186/252 | Train Loss=0.009236211888492107 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 187/252 | Train Loss=0.009556789882481098 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 188/252 | Train Loss=0.00967609602957964 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 189/252 | Train Loss=0.009279814548790455 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 190/252 | Train Loss=0.009415668435394764 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 191/252 | Train Loss=0.009311916306614876 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 192/252 | Train Loss=0.00921050738543272 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 193/252 | Train Loss=0.009165458381175995 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 194/252 | Train Loss=0.00933900848031044 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 195/252 | Train Loss=0.00907096266746521 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 196/252 | Train Loss=0.008978721685707569 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 197/252 | Train Loss=0.009337677620351315 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 198/252 | Train Loss=0.009154172614216805 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 199/252 | Train Loss=0.00915629044175148 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 200/252 | Train Loss=0.008880870416760445 | Train Acc=1.0\n",
            "Running Validation...\n",
            " Valid Loss: 0.0148\n",
            " Valid Accuracy: 0.9992\n",
            " Valid F1 score: 0.9991\n",
            "best_valid_loss = 0.0188\n",
            "eval_loss = 0.0148\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valoss_5epochs.pth\n",
            "best_eval_accuracy = 0.9992\n",
            "eval_accuracy = 0.9992\n",
            " Train Accuracy: 0.9980\n",
            " Train F1 score: 0.9975\n",
            " Train Loss: 0.0187\n",
            "[TRAIN] Epoch 3/5 | Batch 201/252 | Train Loss=0.00914428848773241 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 202/252 | Train Loss=0.011003732681274414 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 203/252 | Train Loss=0.008869301527738571 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 204/252 | Train Loss=0.008731274865567684 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 205/252 | Train Loss=0.009491810575127602 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 206/252 | Train Loss=0.009002592414617538 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 207/252 | Train Loss=0.008578343316912651 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 208/252 | Train Loss=0.008747131563723087 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 209/252 | Train Loss=0.00871421117335558 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 210/252 | Train Loss=0.010162065736949444 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 211/252 | Train Loss=0.008801044896245003 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 212/252 | Train Loss=0.008626407943665981 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 213/252 | Train Loss=0.00876691285520792 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 214/252 | Train Loss=0.008854571729898453 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 215/252 | Train Loss=0.008469662629067898 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 216/252 | Train Loss=0.008953221142292023 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 217/252 | Train Loss=0.008841976523399353 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 218/252 | Train Loss=0.008954937569797039 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 219/252 | Train Loss=0.008377575315535069 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 220/252 | Train Loss=0.008496671915054321 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 221/252 | Train Loss=0.008678155019879341 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 222/252 | Train Loss=0.008516437374055386 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 223/252 | Train Loss=0.008601702749729156 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 224/252 | Train Loss=0.008596021682024002 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 225/252 | Train Loss=0.008509309031069279 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 226/252 | Train Loss=0.008490688167512417 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 227/252 | Train Loss=0.008688494563102722 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 228/252 | Train Loss=0.008523163385689259 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 229/252 | Train Loss=0.008601605892181396 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 230/252 | Train Loss=0.008392399176955223 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 231/252 | Train Loss=0.008494110777974129 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 232/252 | Train Loss=0.008313899859786034 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 233/252 | Train Loss=0.008287130855023861 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 234/252 | Train Loss=0.008498979732394218 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 235/252 | Train Loss=0.008213698863983154 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 236/252 | Train Loss=0.008282155729830265 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 237/252 | Train Loss=0.008141283877193928 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 238/252 | Train Loss=0.008170856162905693 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 239/252 | Train Loss=0.03886016458272934 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 3/5 | Batch 240/252 | Train Loss=0.008293909020721912 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 241/252 | Train Loss=0.008221237920224667 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 242/252 | Train Loss=0.008056736551225185 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 243/252 | Train Loss=0.008198309689760208 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 244/252 | Train Loss=0.008323708549141884 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 245/252 | Train Loss=0.008086551912128925 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 246/252 | Train Loss=0.00817470345646143 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 247/252 | Train Loss=0.008149780333042145 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 248/252 | Train Loss=0.008057592436671257 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 249/252 | Train Loss=0.008191161789000034 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 250/252 | Train Loss=0.00834893248975277 | Train Acc=1.0\n",
            "[TRAIN] Epoch 3/5 | Batch 251/252 | Train Loss=0.007910190150141716 | Train Acc=1.0\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "[TRAIN] Epoch 4/5 | Batch 0/252 | Train Loss=0.013985760509967804 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 1/252 | Train Loss=0.0146834971383214 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 2/252 | Train Loss=0.013168200850486755 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 3/252 | Train Loss=0.013707853853702545 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 4/252 | Train Loss=0.012554256245493889 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 5/252 | Train Loss=0.013243447989225388 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 6/252 | Train Loss=0.04010491818189621 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 4/5 | Batch 7/252 | Train Loss=0.013451539911329746 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 8/252 | Train Loss=0.013722994364798069 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 9/252 | Train Loss=0.013549488969147205 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 10/252 | Train Loss=0.01307766418904066 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 11/252 | Train Loss=0.014033019542694092 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 12/252 | Train Loss=0.013341689482331276 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 13/252 | Train Loss=0.01300020981580019 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 14/252 | Train Loss=0.013717924244701862 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 15/252 | Train Loss=0.012945331633090973 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 16/252 | Train Loss=0.012932502664625645 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 17/252 | Train Loss=0.013461176306009293 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 18/252 | Train Loss=0.012450426816940308 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 19/252 | Train Loss=0.012921038083732128 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 20/252 | Train Loss=0.012493100017309189 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 21/252 | Train Loss=0.011976383626461029 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 22/252 | Train Loss=0.012678496539592743 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 23/252 | Train Loss=0.012645665556192398 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 24/252 | Train Loss=0.012433228082954884 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 25/252 | Train Loss=0.01255595963448286 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 26/252 | Train Loss=0.011810733005404472 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 27/252 | Train Loss=0.012269644066691399 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 28/252 | Train Loss=0.012947385199368 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 29/252 | Train Loss=0.01260403636842966 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 30/252 | Train Loss=0.012136508710682392 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 31/252 | Train Loss=0.012637406587600708 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 32/252 | Train Loss=0.012731991708278656 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 33/252 | Train Loss=0.01249693427234888 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 34/252 | Train Loss=0.011627944186329842 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 35/252 | Train Loss=0.012460988014936447 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 36/252 | Train Loss=0.011986986733973026 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 37/252 | Train Loss=0.20121948421001434 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 4/5 | Batch 38/252 | Train Loss=0.011970201507210732 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 39/252 | Train Loss=0.04481728747487068 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 4/5 | Batch 40/252 | Train Loss=0.0117277717217803 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 41/252 | Train Loss=0.011713267304003239 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 42/252 | Train Loss=0.011416314169764519 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 43/252 | Train Loss=0.012291022576391697 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 44/252 | Train Loss=0.012384695932269096 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 45/252 | Train Loss=0.012296386063098907 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 46/252 | Train Loss=0.013065016828477383 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 47/252 | Train Loss=0.012852713465690613 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 48/252 | Train Loss=0.0120304049924016 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 49/252 | Train Loss=0.01173006184399128 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 50/252 | Train Loss=0.012789235450327396 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 51/252 | Train Loss=0.012121280655264854 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 52/252 | Train Loss=0.013303611427545547 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 53/252 | Train Loss=0.011656631715595722 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 54/252 | Train Loss=0.01236192137002945 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 55/252 | Train Loss=0.011961189098656178 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 56/252 | Train Loss=0.011768418364226818 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 57/252 | Train Loss=0.011114981956779957 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 58/252 | Train Loss=0.011495658196508884 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 59/252 | Train Loss=0.011719346977770329 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 60/252 | Train Loss=0.010937891900539398 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 61/252 | Train Loss=0.012202606536448002 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 62/252 | Train Loss=0.014403018169105053 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 63/252 | Train Loss=0.011481321416795254 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 64/252 | Train Loss=0.011513815261423588 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 65/252 | Train Loss=0.01080491952598095 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 66/252 | Train Loss=0.011544428765773773 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 67/252 | Train Loss=0.011220530606806278 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 68/252 | Train Loss=0.012132515199482441 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 69/252 | Train Loss=0.011306212283670902 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 70/252 | Train Loss=0.011196720413863659 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 71/252 | Train Loss=0.010709394700825214 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 72/252 | Train Loss=0.011393826454877853 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 73/252 | Train Loss=0.012026000767946243 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 74/252 | Train Loss=0.010731691494584084 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 75/252 | Train Loss=0.011203765869140625 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 76/252 | Train Loss=0.19854684174060822 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 4/5 | Batch 77/252 | Train Loss=0.011591644957661629 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 78/252 | Train Loss=0.01076462958008051 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 79/252 | Train Loss=0.010999374091625214 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 80/252 | Train Loss=0.010873822495341301 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 81/252 | Train Loss=0.011632474139332771 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 82/252 | Train Loss=0.12380129843950272 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 4/5 | Batch 83/252 | Train Loss=0.010570700280368328 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 84/252 | Train Loss=0.010846308432519436 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 85/252 | Train Loss=0.01109748613089323 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 86/252 | Train Loss=0.010389233008027077 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 87/252 | Train Loss=0.010407844558358192 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 88/252 | Train Loss=0.010120077058672905 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 89/252 | Train Loss=0.010695608332753181 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 90/252 | Train Loss=0.15108653903007507 | Train Acc=0.96875\n",
            "[TRAIN] Epoch 4/5 | Batch 91/252 | Train Loss=0.011825940571725368 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 92/252 | Train Loss=0.01253143697977066 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 93/252 | Train Loss=0.011302720755338669 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 94/252 | Train Loss=0.011123701930046082 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 95/252 | Train Loss=0.011673825792968273 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 96/252 | Train Loss=0.010230214335024357 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 97/252 | Train Loss=0.010630100034177303 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 98/252 | Train Loss=0.011156548745930195 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 99/252 | Train Loss=0.010160327889025211 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 100/252 | Train Loss=0.011282372288405895 | Train Acc=1.0\n",
            "Running Validation...\n",
            " Valid Loss: 0.0102\n",
            " Valid Accuracy: 0.9992\n",
            " Valid F1 score: 0.9991\n",
            "best_valid_loss = 0.0148\n",
            "eval_loss = 0.0102\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valoss_5epochs.pth\n",
            "best_eval_accuracy = 0.9992\n",
            "eval_accuracy = 0.9992\n",
            " Train Accuracy: 0.9981\n",
            " Train F1 score: 0.9964\n",
            " Train Loss: 0.0076\n",
            "[TRAIN] Epoch 4/5 | Batch 101/252 | Train Loss=0.006228565238416195 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 102/252 | Train Loss=0.006299677770584822 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 103/252 | Train Loss=0.00620233453810215 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 104/252 | Train Loss=0.006227411329746246 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 105/252 | Train Loss=0.00620441697537899 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 106/252 | Train Loss=0.006305461749434471 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 107/252 | Train Loss=0.006266883108764887 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 108/252 | Train Loss=0.006096203345805407 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 109/252 | Train Loss=0.00614461163058877 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 110/252 | Train Loss=0.006016567349433899 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 111/252 | Train Loss=0.006300747860223055 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 112/252 | Train Loss=0.006096580997109413 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 113/252 | Train Loss=0.006138932891190052 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 114/252 | Train Loss=0.006098070647567511 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 115/252 | Train Loss=0.0060425205156207085 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 116/252 | Train Loss=0.005970223341137171 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 117/252 | Train Loss=0.006008491851389408 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 118/252 | Train Loss=0.006015080958604813 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 119/252 | Train Loss=0.00593614112585783 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 120/252 | Train Loss=0.0059367879293859005 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 121/252 | Train Loss=0.005863477475941181 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 122/252 | Train Loss=0.0058990176767110825 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 123/252 | Train Loss=0.006043809000402689 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 124/252 | Train Loss=0.006141061894595623 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 125/252 | Train Loss=0.0059015764854848385 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 126/252 | Train Loss=0.005890383385121822 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 127/252 | Train Loss=0.00596202677115798 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 128/252 | Train Loss=0.005910979583859444 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 129/252 | Train Loss=0.005782033782452345 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 130/252 | Train Loss=0.005952569656074047 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 131/252 | Train Loss=0.00598209910094738 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 132/252 | Train Loss=0.005834495648741722 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 133/252 | Train Loss=0.005835875868797302 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 134/252 | Train Loss=0.005941291805356741 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 135/252 | Train Loss=0.005767428781837225 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 136/252 | Train Loss=0.005691952537745237 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 137/252 | Train Loss=0.005819736514240503 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 138/252 | Train Loss=0.005686159711331129 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 139/252 | Train Loss=0.005629346240311861 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 140/252 | Train Loss=0.005700852256268263 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 141/252 | Train Loss=0.005659274756908417 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 142/252 | Train Loss=0.005804855842143297 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 143/252 | Train Loss=0.0056938002817332745 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 144/252 | Train Loss=0.00578997703269124 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 145/252 | Train Loss=0.0056673879735171795 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 146/252 | Train Loss=0.005562178790569305 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 147/252 | Train Loss=0.005739790387451649 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 148/252 | Train Loss=0.005571442656219006 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 149/252 | Train Loss=0.005814429838210344 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 150/252 | Train Loss=0.005592259578406811 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 151/252 | Train Loss=0.00553083186969161 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 152/252 | Train Loss=0.005483474116772413 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 153/252 | Train Loss=0.005551788490265608 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 154/252 | Train Loss=0.0054951999336481094 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 155/252 | Train Loss=0.005495698656886816 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 156/252 | Train Loss=0.005666297394782305 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 157/252 | Train Loss=0.005500557832419872 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 158/252 | Train Loss=0.005628813989460468 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 159/252 | Train Loss=0.005571375600993633 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 160/252 | Train Loss=0.0054210033267736435 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 161/252 | Train Loss=0.005670392420142889 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 162/252 | Train Loss=0.005645796190947294 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 163/252 | Train Loss=0.005666224751621485 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 164/252 | Train Loss=0.005316681694239378 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 165/252 | Train Loss=0.005436705891042948 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 166/252 | Train Loss=0.005423633847385645 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 167/252 | Train Loss=0.0054636504501104355 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 168/252 | Train Loss=0.0054156724363565445 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 169/252 | Train Loss=0.005424846895039082 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 170/252 | Train Loss=0.005483585875481367 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 171/252 | Train Loss=0.005571550689637661 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 172/252 | Train Loss=0.0053566317074000835 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 173/252 | Train Loss=0.005387277342379093 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 174/252 | Train Loss=0.005352400708943605 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 175/252 | Train Loss=0.005457093473523855 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 176/252 | Train Loss=0.005628887098282576 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 177/252 | Train Loss=0.0054552904330194 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 178/252 | Train Loss=0.0053746760822832584 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 179/252 | Train Loss=0.005350117571651936 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 180/252 | Train Loss=0.005252791102975607 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 181/252 | Train Loss=0.005180011037737131 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 182/252 | Train Loss=0.0053179324604570866 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 183/252 | Train Loss=0.005341474898159504 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 184/252 | Train Loss=0.005302457604557276 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 185/252 | Train Loss=0.005422152578830719 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 186/252 | Train Loss=0.005244018509984016 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 187/252 | Train Loss=0.005368529353290796 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 188/252 | Train Loss=0.0053389365784823895 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 189/252 | Train Loss=0.005229071248322725 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 190/252 | Train Loss=0.005302012898027897 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 191/252 | Train Loss=0.005353892687708139 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 192/252 | Train Loss=0.005263928789645433 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 193/252 | Train Loss=0.005273202899843454 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 194/252 | Train Loss=0.005304326303303242 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 195/252 | Train Loss=0.005167385097593069 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 196/252 | Train Loss=0.005103207193315029 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 197/252 | Train Loss=0.005281453486531973 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 198/252 | Train Loss=0.005202048923820257 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 199/252 | Train Loss=0.005277566611766815 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 200/252 | Train Loss=0.005145899951457977 | Train Acc=1.0\n",
            "Running Validation...\n",
            " Valid Loss: 0.0098\n",
            " Valid Accuracy: 0.9992\n",
            " Valid F1 score: 0.9991\n",
            "best_valid_loss = 0.0102\n",
            "eval_loss = 0.0098\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valoss_5epochs.pth\n",
            "best_eval_accuracy = 0.9992\n",
            "eval_accuracy = 0.9992\n",
            " Train Accuracy: 0.9991\n",
            " Train F1 score: 0.9982\n",
            " Train Loss: 0.0098\n",
            "[TRAIN] Epoch 4/5 | Batch 201/252 | Train Loss=0.005272072274237871 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 202/252 | Train Loss=0.005293800495564938 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 203/252 | Train Loss=0.005094984546303749 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 204/252 | Train Loss=0.005068358965218067 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 205/252 | Train Loss=0.005258636083453894 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 206/252 | Train Loss=0.005234011448919773 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 207/252 | Train Loss=0.004989519249647856 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 208/252 | Train Loss=0.0051146140322089195 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 209/252 | Train Loss=0.005096079781651497 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 210/252 | Train Loss=0.00511346198618412 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 211/252 | Train Loss=0.005139978602528572 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 212/252 | Train Loss=0.005061205942183733 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 213/252 | Train Loss=0.005122738424688578 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 214/252 | Train Loss=0.005175088532269001 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 215/252 | Train Loss=0.0049909306690096855 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 216/252 | Train Loss=0.005217823199927807 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 217/252 | Train Loss=0.005158874671906233 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 218/252 | Train Loss=0.005109213292598724 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 219/252 | Train Loss=0.004946341272443533 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 220/252 | Train Loss=0.004993315786123276 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 221/252 | Train Loss=0.0050881230272352695 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 222/252 | Train Loss=0.005017867311835289 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 223/252 | Train Loss=0.005070696584880352 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 224/252 | Train Loss=0.005036931484937668 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 225/252 | Train Loss=0.005006522871553898 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 226/252 | Train Loss=0.00503736175596714 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 227/252 | Train Loss=0.0050966436974704266 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 228/252 | Train Loss=0.005120473448187113 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 229/252 | Train Loss=0.0050579942762851715 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 230/252 | Train Loss=0.0049731312319636345 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 231/252 | Train Loss=0.004827873315662146 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 232/252 | Train Loss=0.004936424549669027 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 233/252 | Train Loss=0.004918031860142946 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 234/252 | Train Loss=0.004980641882866621 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 235/252 | Train Loss=0.004907872527837753 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 236/252 | Train Loss=0.0049290950410068035 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 237/252 | Train Loss=0.004889559932053089 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 238/252 | Train Loss=0.004874784965068102 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 239/252 | Train Loss=0.005166420713067055 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 240/252 | Train Loss=0.004975331947207451 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 241/252 | Train Loss=0.004931588191539049 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 242/252 | Train Loss=0.004813553299754858 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 243/252 | Train Loss=0.004915722645819187 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 244/252 | Train Loss=0.004991905763745308 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 245/252 | Train Loss=0.00482009444385767 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 246/252 | Train Loss=0.0048958552069962025 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 247/252 | Train Loss=0.0049118585884571075 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 248/252 | Train Loss=0.004902235232293606 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 249/252 | Train Loss=0.0049107796512544155 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 250/252 | Train Loss=0.004999994765967131 | Train Acc=1.0\n",
            "[TRAIN] Epoch 4/5 | Batch 251/252 | Train Loss=0.004755440633744001 | Train Acc=1.0\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "[TRAIN] Epoch 5/5 | Batch 0/252 | Train Loss=0.007896366529166698 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 1/252 | Train Loss=0.007657351437956095 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 2/252 | Train Loss=0.008317822590470314 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 3/252 | Train Loss=0.008581255562603474 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 4/252 | Train Loss=0.00797946099191904 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 5/252 | Train Loss=0.008269810117781162 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 6/252 | Train Loss=0.008365187793970108 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 7/252 | Train Loss=0.007685517892241478 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 8/252 | Train Loss=0.008009823970496655 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 9/252 | Train Loss=0.008155918680131435 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 10/252 | Train Loss=0.007844222709536552 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 11/252 | Train Loss=0.008803148753941059 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 12/252 | Train Loss=0.008545157499611378 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 13/252 | Train Loss=0.008020147681236267 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 14/252 | Train Loss=0.008043574169278145 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 15/252 | Train Loss=0.00854497216641903 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 16/252 | Train Loss=0.007826246321201324 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 17/252 | Train Loss=0.008190417662262917 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 18/252 | Train Loss=0.007801966741681099 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 19/252 | Train Loss=0.007833675481379032 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 20/252 | Train Loss=0.007969929836690426 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 21/252 | Train Loss=0.007932321168482304 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 22/252 | Train Loss=0.007937888614833355 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 23/252 | Train Loss=0.007729243487119675 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 24/252 | Train Loss=0.00884775910526514 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 25/252 | Train Loss=0.007536532822996378 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 26/252 | Train Loss=0.0076548028737306595 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 27/252 | Train Loss=0.007747827097773552 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 28/252 | Train Loss=0.008116361685097218 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 29/252 | Train Loss=0.007611187174916267 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 30/252 | Train Loss=0.00773588614538312 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 31/252 | Train Loss=0.008234899491071701 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 32/252 | Train Loss=0.007926852442324162 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 33/252 | Train Loss=0.006852148100733757 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 34/252 | Train Loss=0.0074138278141617775 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 35/252 | Train Loss=0.007764806039631367 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 36/252 | Train Loss=0.007351666688919067 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 37/252 | Train Loss=0.008198298513889313 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 38/252 | Train Loss=0.007612163666635752 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 39/252 | Train Loss=0.0076592350378632545 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 40/252 | Train Loss=0.007660471834242344 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 41/252 | Train Loss=0.0076452502980828285 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 42/252 | Train Loss=0.007519037462770939 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 43/252 | Train Loss=0.007410318125039339 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 44/252 | Train Loss=0.007163556292653084 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 45/252 | Train Loss=0.006964303553104401 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 46/252 | Train Loss=0.007256112992763519 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 47/252 | Train Loss=0.008163605816662312 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 48/252 | Train Loss=0.007314634509384632 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 49/252 | Train Loss=0.011170681565999985 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 50/252 | Train Loss=0.007696122396737337 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 51/252 | Train Loss=0.00792527012526989 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 52/252 | Train Loss=0.007428694516420364 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 53/252 | Train Loss=0.007679159753024578 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 54/252 | Train Loss=0.007414897903800011 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 55/252 | Train Loss=0.007127271965146065 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 56/252 | Train Loss=0.008118345402181149 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 57/252 | Train Loss=0.00732075423002243 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 58/252 | Train Loss=0.0077085127122700214 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 59/252 | Train Loss=0.007562300655990839 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 60/252 | Train Loss=0.007296659052371979 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 61/252 | Train Loss=0.007761204615235329 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 62/252 | Train Loss=0.007583681493997574 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 63/252 | Train Loss=0.007966923527419567 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 64/252 | Train Loss=0.007818203419446945 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 65/252 | Train Loss=0.007349025923758745 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 66/252 | Train Loss=0.007055048365145922 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 67/252 | Train Loss=0.006895496975630522 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 68/252 | Train Loss=0.007236639503389597 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 69/252 | Train Loss=0.007389870006591082 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 70/252 | Train Loss=0.007150206249207258 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 71/252 | Train Loss=0.006883337162435055 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 72/252 | Train Loss=0.0071382164023816586 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 73/252 | Train Loss=0.006441361736506224 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 74/252 | Train Loss=0.0075731705874204636 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 75/252 | Train Loss=0.007094675675034523 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 76/252 | Train Loss=0.006627553608268499 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 77/252 | Train Loss=0.0071512265130877495 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 78/252 | Train Loss=0.006789017468690872 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 79/252 | Train Loss=0.006785732693970203 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 80/252 | Train Loss=0.007285377476364374 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 81/252 | Train Loss=0.007000384386628866 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 82/252 | Train Loss=0.00655833724886179 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 83/252 | Train Loss=0.006614529527723789 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 84/252 | Train Loss=0.006801079027354717 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 85/252 | Train Loss=0.006704504601657391 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 86/252 | Train Loss=0.017999637871980667 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 87/252 | Train Loss=0.006359864491969347 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 88/252 | Train Loss=0.006681961938738823 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 89/252 | Train Loss=0.006976620759814978 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 90/252 | Train Loss=0.0072585889138281345 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 91/252 | Train Loss=0.007738560903817415 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 92/252 | Train Loss=0.006389269605278969 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 93/252 | Train Loss=0.007222775369882584 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 94/252 | Train Loss=0.006546991877257824 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 95/252 | Train Loss=0.0065839276649057865 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 96/252 | Train Loss=0.006178267765790224 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 97/252 | Train Loss=0.007017042953521013 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 98/252 | Train Loss=0.007000780664384365 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 99/252 | Train Loss=0.0069617838598787785 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 100/252 | Train Loss=0.006825663149356842 | Train Acc=1.0\n",
            "Running Validation...\n",
            " Valid Loss: 0.0085\n",
            " Valid Accuracy: 0.9992\n",
            " Valid F1 score: 0.9991\n",
            "best_valid_loss = 0.0098\n",
            "eval_loss = 0.0085\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valoss_5epochs.pth\n",
            "best_eval_accuracy = 0.9992\n",
            "eval_accuracy = 0.9992\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_5epoch.pth\n",
            " Train Accuracy: 1.0000\n",
            " Train F1 score: 1.0000\n",
            " Train Loss: 0.0031\n",
            "[TRAIN] Epoch 5/5 | Batch 101/252 | Train Loss=0.003935088403522968 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 102/252 | Train Loss=0.0039980108849704266 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 103/252 | Train Loss=0.003928394988179207 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 104/252 | Train Loss=0.003970389720052481 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 105/252 | Train Loss=0.003945276141166687 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 106/252 | Train Loss=0.003957654349505901 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 107/252 | Train Loss=0.003966417629271746 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 108/252 | Train Loss=0.0038895357865840197 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 109/252 | Train Loss=0.003924455959349871 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 110/252 | Train Loss=0.003828542772680521 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 111/252 | Train Loss=0.003952283412218094 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 112/252 | Train Loss=0.0038805126678198576 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 113/252 | Train Loss=0.003911407198756933 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 114/252 | Train Loss=0.0038904864341020584 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 115/252 | Train Loss=0.003864264814183116 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 116/252 | Train Loss=0.0038094588089734316 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 117/252 | Train Loss=0.0038486693520098925 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 118/252 | Train Loss=0.0038750325329601765 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 119/252 | Train Loss=0.003836624324321747 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 120/252 | Train Loss=0.0038110874593257904 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 121/252 | Train Loss=0.003768632421270013 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 122/252 | Train Loss=0.003797251498326659 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 123/252 | Train Loss=0.0038809862453490496 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 124/252 | Train Loss=0.003932102583348751 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 125/252 | Train Loss=0.003783439751714468 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 126/252 | Train Loss=0.0037907110527157784 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 127/252 | Train Loss=0.0038220302667468786 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 128/252 | Train Loss=0.0038356538861989975 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 129/252 | Train Loss=0.0037576514296233654 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 130/252 | Train Loss=0.0038629984483122826 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 131/252 | Train Loss=0.003846871666610241 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 132/252 | Train Loss=0.003771047107875347 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 133/252 | Train Loss=0.003777101170271635 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 134/252 | Train Loss=0.003755930345505476 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 135/252 | Train Loss=0.003740910207852721 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 136/252 | Train Loss=0.0036942502483725548 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 137/252 | Train Loss=0.003773455275222659 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 138/252 | Train Loss=0.003695772960782051 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 139/252 | Train Loss=0.003662761999294162 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 140/252 | Train Loss=0.0037199899088591337 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 141/252 | Train Loss=0.0036656155716627836 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 142/252 | Train Loss=0.0037676426582038403 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 143/252 | Train Loss=0.003692932892590761 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 144/252 | Train Loss=0.0037398075219243765 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 145/252 | Train Loss=0.003701904322952032 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 146/252 | Train Loss=0.003628156613558531 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 147/252 | Train Loss=0.003735561389476061 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 148/252 | Train Loss=0.0036371375899761915 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 149/252 | Train Loss=0.0037817563861608505 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 150/252 | Train Loss=0.00365641713142395 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 151/252 | Train Loss=0.0036218438763171434 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 152/252 | Train Loss=0.003592895111069083 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 153/252 | Train Loss=0.003634283086284995 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 154/252 | Train Loss=0.003595946589484811 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 155/252 | Train Loss=0.003607898484915495 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 156/252 | Train Loss=0.0037000959273427725 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 157/252 | Train Loss=0.003615095978602767 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 158/252 | Train Loss=0.0036782058887183666 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 159/252 | Train Loss=0.0036647168453782797 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 160/252 | Train Loss=0.003566670697182417 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 161/252 | Train Loss=0.0036681853234767914 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 162/252 | Train Loss=0.003713493002578616 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 163/252 | Train Loss=0.0036794247571378946 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 164/252 | Train Loss=0.003498111618682742 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 165/252 | Train Loss=0.0035683305468410254 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 166/252 | Train Loss=0.003576788119971752 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 167/252 | Train Loss=0.0035777471493929625 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 168/252 | Train Loss=0.0035590827465057373 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 169/252 | Train Loss=0.003554561408236623 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 170/252 | Train Loss=0.0036020372062921524 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 171/252 | Train Loss=0.0036421034019440413 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 172/252 | Train Loss=0.003543207421898842 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 173/252 | Train Loss=0.003546726191416383 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 174/252 | Train Loss=0.0035330301616340876 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 175/252 | Train Loss=0.0035890312865376472 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 176/252 | Train Loss=0.0036285663954913616 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 177/252 | Train Loss=0.003602368989959359 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 178/252 | Train Loss=0.0035449983552098274 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 179/252 | Train Loss=0.0035442898515611887 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 180/252 | Train Loss=0.0034789033234119415 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 181/252 | Train Loss=0.003430115757510066 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 182/252 | Train Loss=0.003522715298458934 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 183/252 | Train Loss=0.00352208549156785 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 184/252 | Train Loss=0.003499460406601429 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 185/252 | Train Loss=0.003589468076825142 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 186/252 | Train Loss=0.0034698559902608395 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 187/252 | Train Loss=0.0035585486330091953 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 188/252 | Train Loss=0.0035311481915414333 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 189/252 | Train Loss=0.0034733174834400415 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 190/252 | Train Loss=0.003523085964843631 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 191/252 | Train Loss=0.0035384579095989466 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 192/252 | Train Loss=0.0034952403511852026 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 193/252 | Train Loss=0.0035078905057162046 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 194/252 | Train Loss=0.0035231406800448895 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 195/252 | Train Loss=0.0034401603043079376 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 196/252 | Train Loss=0.003403370501473546 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 197/252 | Train Loss=0.003508694237098098 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 198/252 | Train Loss=0.0034549450501799583 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 199/252 | Train Loss=0.0035136474762111902 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 200/252 | Train Loss=0.0034376720432192087 | Train Acc=1.0\n",
            "Running Validation...\n",
            " Valid Loss: 0.0081\n",
            " Valid Accuracy: 0.9992\n",
            " Valid F1 score: 0.9991\n",
            "best_valid_loss = 0.0085\n",
            "eval_loss = 0.0081\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_best_valoss_5epochs.pth\n",
            "best_eval_accuracy = 0.9992\n",
            "eval_accuracy = 0.9992\n",
            "Model saved to ==> 03-10-2022_07-37-24/model_5epoch.pth\n",
            " Train Accuracy: 1.0000\n",
            " Train F1 score: 1.0000\n",
            " Train Loss: 0.0045\n",
            "[TRAIN] Epoch 5/5 | Batch 201/252 | Train Loss=0.0035161415580660105 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 202/252 | Train Loss=0.0035286922939121723 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 203/252 | Train Loss=0.003408190095797181 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 204/252 | Train Loss=0.0033912360668182373 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 205/252 | Train Loss=0.003453468205407262 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 206/252 | Train Loss=0.0034784539602696896 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 207/252 | Train Loss=0.0033381865359842777 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 208/252 | Train Loss=0.003403416834771633 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 209/252 | Train Loss=0.003414598060771823 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 210/252 | Train Loss=0.00341520132496953 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 211/252 | Train Loss=0.003420674940571189 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 212/252 | Train Loss=0.0033823379781097174 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 213/252 | Train Loss=0.003425207920372486 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 214/252 | Train Loss=0.003462987020611763 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 215/252 | Train Loss=0.0033455113880336285 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 216/252 | Train Loss=0.0035002364311367273 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 217/252 | Train Loss=0.0034572870936244726 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 218/252 | Train Loss=0.0034070443361997604 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 219/252 | Train Loss=0.003330521984025836 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 220/252 | Train Loss=0.0033505382016301155 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 221/252 | Train Loss=0.003409801283851266 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 222/252 | Train Loss=0.0033679031766951084 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 223/252 | Train Loss=0.0034082960337400436 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 224/252 | Train Loss=0.0033897929824888706 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 225/252 | Train Loss=0.0033712636213749647 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 226/252 | Train Loss=0.0033864614088088274 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 227/252 | Train Loss=0.003437709528952837 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 228/252 | Train Loss=0.003379291156306863 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 229/252 | Train Loss=0.0033908530604094267 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 230/252 | Train Loss=0.0033505589235574007 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 231/252 | Train Loss=0.0032596420496702194 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 232/252 | Train Loss=0.0033248153049498796 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 233/252 | Train Loss=0.0033164825290441513 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 234/252 | Train Loss=0.0033470646012574434 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 235/252 | Train Loss=0.0033082691952586174 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 236/252 | Train Loss=0.003314660396426916 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 237/252 | Train Loss=0.003306362545117736 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 238/252 | Train Loss=0.0032828557305037975 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 239/252 | Train Loss=0.003432261059060693 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 240/252 | Train Loss=0.0033497486729174852 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 241/252 | Train Loss=0.003333413042128086 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 242/252 | Train Loss=0.0032545647118240595 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 243/252 | Train Loss=0.0033211344853043556 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 244/252 | Train Loss=0.0033656589221209288 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 245/252 | Train Loss=0.0032656844705343246 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 246/252 | Train Loss=0.0033047606702893972 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 247/252 | Train Loss=0.0033124065957963467 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 248/252 | Train Loss=0.003295747796073556 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 249/252 | Train Loss=0.0033166061621159315 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 250/252 | Train Loss=0.0033715625759214163 | Train Acc=1.0\n",
            "[TRAIN] Epoch 5/5 | Batch 251/252 | Train Loss=0.0032290725503116846 | Train Acc=1.0\n",
            "Training complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classes = ['__label__sống_trẻ', '__label__thời_sự', '__label__công_nghệ', '__label__sức_khỏe', '__label__giáo_dục', '__label__xe_360', '__label__thời_trang', '__label__du_lịch', '__label__âm_nhạc', '__label__xuất_bản', '__label__nhịp_sống', '__label__kinh_doanh', '__label__pháp_luật', '__label__ẩm_thực', '__label__thế_giới', '__label__thể_thao', '__label__giải_trí', '__label__phim_ảnh']\n",
        "\n",
        "# Load model from saved file\n",
        "load_model = torch.load('/content/drive/MyDrive/News/PhoBert_News_Classification/03-10-2022_07-23-14/model_5epoch.pth')\n",
        "model = BERTClassifier(len(classes))\n",
        "model.load_state_dict(load_model['model_state_dict']) # Load state dict, include input_ids and attention_mask"
      ],
      "metadata": {
        "id": "2xql3VH6MkDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c6d9533-8c40-4638-aef1-35ae89e85819"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOAD BERT PRETRAIN MODEL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./vinai/phobert-base/pytorch_model.bin were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./vinai/phobert-base/pytorch_model.bin and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Ô tô Trung Quốc vào Việt Nam từ cách đây 2 thập niên và đến nay vẫn bị mang tiếng \"nhái\" thiết kế nhiều thương hiệu tên tuổi. Nhắc đến ô tô Trung Quốc, bên cạnh chất lượng bị nghi ngờ thì trong suy nghĩ và định kiến của người Việt, nhiều mẫu xe đến từ bên kia biên giới không có bản sắc mà thường \"nhái\" theo các thương hiệu tên tuổi trên thế giới. Kể từ khi xe Trung Quốc vào Việt Nam cách đây hơn 2 thập niên và đến hiện tại, yếu tố nhái kiểu dáng vẫn còn tồn tại. Ưu điểm của những xe nhái thiết kế là có giá rẻ hơn rất nhiều so với xe thật. Cho dù công nghệ, vận hành khó có thể bằng nhưng một số mẫu ô tô nhái của Trung Quốc vẫn có phân khúc khách hàng riêng. Dưới đây là một số mẫu xe điển hình như vậy đã bán ở Việt Nam.'\n",
        "bert_classifier_trainer.predict_text(classes=classes, text=text, tokenizer=tokenizer, max_len=256)"
      ],
      "metadata": {
        "id": "sbyGLYJc_cd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4741097d-f64e-4799-8e62-8d8ac6ffe1d7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PREDICT] \n",
            "TEXT: Ô tô Trung Quốc vào Việt Nam từ cách đây 2 thập niên và đến nay vẫn bị mang tiếng \"nhái\" thiết kế nhiều thương hiệu tên tuổi. Nhắc đến ô tô Trung Quốc, bên cạnh chất lượng bị nghi ngờ thì trong suy nghĩ và định kiến của người Việt, nhiều mẫu xe đến từ bên kia biên giới không có bản sắc mà thường \"nhái\" theo các thương hiệu tên tuổi trên thế giới. Kể từ khi xe Trung Quốc vào Việt Nam cách đây hơn 2 thập niên và đến hiện tại, yếu tố nhái kiểu dáng vẫn còn tồn tại. Ưu điểm của những xe nhái thiết kế là có giá rẻ hơn rất nhiều so với xe thật. Cho dù công nghệ, vận hành khó có thể bằng nhưng một số mẫu ô tô nhái của Trung Quốc vẫn có phân khúc khách hàng riêng. Dưới đây là một số mẫu xe điển hình như vậy đã bán ở Việt Nam.\n",
            "LABEL: __label__xe_360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-vTeL85xK0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}